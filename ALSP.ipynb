{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Victhagas/EEG-Analysis/blob/main/ALSP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading libraries"
      ],
      "metadata": {
        "id": "A-Bu5LEjbhoR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "JMwy6TWsDnI1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading data"
      ],
      "metadata": {
        "id": "aCnP3FCjblxd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = pd.read_csv(\"READYCPTfourthrun.csv\")"
      ],
      "metadata": {
        "id": "8bsHXNX-D9js"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Separating matrix of features from the dependent variable"
      ],
      "metadata": {
        "id": "H9lMXb9vbwsL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = dataset.iloc[:, :-1].values\n",
        "y = dataset.iloc[:, -1].values"
      ],
      "metadata": {
        "id": "PTzymbgCbvNj"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x"
      ],
      "metadata": {
        "id": "_s1ZhjtA7u9t",
        "outputId": "aed89853-2b53-497c-8803-f2c728881b1e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1.00000000e+00, -3.21006938e-01,  1.45900342e-01,\n",
              "         2.83669934e-01,  3.75000391e-01, -1.36040051e+00,\n",
              "        -1.39985980e+00, -1.46684067e+00, -1.61392729e+00,\n",
              "         6.98692337e-01,  1.01106286e+00,  1.48115942e+00,\n",
              "         2.30816212e+00,  1.45503210e+00,  9.10777348e-01,\n",
              "         9.04378906e-01,  1.37907547e+00],\n",
              "       [ 1.00000000e+00,  8.38200127e-01,  9.00155901e-01,\n",
              "        -2.86074610e-02,  3.66858873e-01,  1.52620208e+00,\n",
              "         1.57247875e-01,  2.99182413e-01,  8.44643214e-01,\n",
              "        -3.45342155e-01,  4.24193242e-01, -4.04199605e-01,\n",
              "        -4.27386895e-01, -1.41536476e+00, -8.05983109e-01,\n",
              "        -2.71176662e-01, -8.21237865e-01],\n",
              "       [ 1.00000000e+00,  6.31221291e-01,  1.41228936e-01,\n",
              "         2.56051863e-01,  8.28995288e-02,  1.05531749e+00,\n",
              "        -8.75903532e-01, -5.60909250e-01, -4.42961671e-01,\n",
              "        -2.79916621e-01,  4.72181114e-01,  4.53073312e-01,\n",
              "         4.25984795e-01, -1.05628926e+00,  5.49874613e-01,\n",
              "         1.29334506e-01,  2.26182060e-01],\n",
              "       [ 1.00000000e+00, -1.82141278e+00, -6.30808838e-01,\n",
              "        -8.77704969e-01,  2.49481066e-01, -7.21762683e-01,\n",
              "        -9.01431004e-01, -2.96571280e-01,  1.56652271e-01,\n",
              "        -8.48695425e-01,  2.12120091e-01, -5.29536417e-01,\n",
              "         8.95075321e-02,  1.55330901e+00,  9.30469094e-01,\n",
              "         7.68281165e-01, -2.48347359e-01],\n",
              "       [ 0.00000000e+00, -1.12472721e+00, -9.37017844e-01,\n",
              "        -1.18374596e+00, -6.04176447e-01,  1.49710425e-01,\n",
              "         2.85099174e-01,  8.60821612e-02, -1.21834958e+00,\n",
              "        -1.38062204e+00, -1.17870931e+00, -1.18391332e+00,\n",
              "         6.11472979e-01,  7.26212219e-01,  3.90042381e-01,\n",
              "         5.93239533e-01,  1.25980151e+00],\n",
              "       [ 0.00000000e+00,  3.28463389e-01, -8.94024646e-01,\n",
              "        -5.44964335e-01,  3.16213896e-01,  1.12593510e+00,\n",
              "         5.20965774e-01,  1.95505116e+00,  2.81152764e+00,\n",
              "        -7.50635236e-01, -1.26559997e+00, -1.74134736e+00,\n",
              "        -1.47648065e+00, -7.26234784e-01,  2.45055463e-01,\n",
              "        -6.94971543e-01, -1.61683483e+00],\n",
              "       [ 0.00000000e+00, -1.62672252e+00, -1.60036162e+00,\n",
              "        -2.76502879e-01, -5.05994988e-01, -4.99643738e-01,\n",
              "        -6.58537069e-01, -1.10500334e+00, -1.10779556e+00,\n",
              "        -8.56081573e-01, -9.50055756e-01,  7.33188609e-01,\n",
              "         7.41363320e-01,  1.33810443e+00,  1.33652751e+00,\n",
              "         1.10127354e+00,  1.20853015e+00],\n",
              "       [ 1.00000000e+00, -2.95607864e+00, -2.53920541e+00,\n",
              "        -2.52427590e+00, -1.62781662e+00,  7.36567411e-02,\n",
              "         1.23426427e-01, -3.30638858e-01,  5.34860996e-01,\n",
              "        -2.21819042e+00, -2.05780558e+00, -1.72912680e+00,\n",
              "        -1.85183772e+00,  1.61308642e+00,  1.31927480e+00,\n",
              "         1.78317148e+00,  6.09894019e-01],\n",
              "       [ 1.00000000e+00, -5.40031781e-01,  8.24311272e-01,\n",
              "         3.21576962e-01, -2.54521165e-01,  8.77994370e-01,\n",
              "         1.74241919e+00,  3.77016895e-01,  3.44912531e-01,\n",
              "        -1.30094679e+00, -4.46497609e-01, -2.23499795e-01,\n",
              "        -5.97918588e-01,  6.05055133e-02, -1.44023873e+00,\n",
              "        -4.90613461e-01, -1.19470828e-01],\n",
              "       [ 1.00000000e+00, -1.58627373e+00, -1.40557576e+00,\n",
              "        -2.10161769e+00, -7.84668486e-01,  1.00525053e+00,\n",
              "         1.06072368e+00,  8.00544353e-01,  8.09121503e-01,\n",
              "        -2.15092000e+00, -1.89193658e+00, -2.26638502e+00,\n",
              "        -1.30764351e+00,  5.25948907e-01,  2.72156625e-01,\n",
              "         7.86733502e-01, -2.99544863e-02],\n",
              "       [ 1.00000000e+00, -8.12641816e-01, -3.87817433e-01,\n",
              "         8.06648332e-01,  5.99449903e-01, -1.81623657e-02,\n",
              "        -7.14140527e-01,  4.86521365e-01,  3.82461026e-01,\n",
              "        -9.48088476e-01, -4.76866421e-02,  8.06562623e-02,\n",
              "         1.78478613e-02,  5.63064891e-01,  7.27814995e-01,\n",
              "        -9.27719112e-01, -7.54914655e-01],\n",
              "       [ 0.00000000e+00,  2.83901670e-01,  7.10434983e-01,\n",
              "         9.68305813e-01,  1.79809625e+00,  1.07204717e+00,\n",
              "         8.44435278e-01,  1.42686653e+00,  1.37476364e+00,\n",
              "        -5.03329591e-01, -1.90807504e-01, -1.54438805e-01,\n",
              "         5.07452844e-01, -8.26626997e-01, -9.62750791e-01,\n",
              "        -1.47153146e+00, -2.00806552e+00],\n",
              "       [ 0.00000000e+00,  7.21924229e-01,  6.71542307e-01,\n",
              "         7.22380455e-02,  9.18717338e-01, -3.38307115e-01,\n",
              "         6.41473434e-01,  7.18157721e-01,  1.50073030e+00,\n",
              "         9.76687569e-01,  5.14954991e-02, -5.19978421e-01,\n",
              "        -2.39017491e-01, -5.78785212e-01, -8.48723131e-01,\n",
              "        -4.95086995e-01, -1.48750647e+00],\n",
              "       [ 0.00000000e+00, -6.10836120e-01, -7.87440119e-01,\n",
              "        -1.00187544e+00, -1.35964508e-01,  5.61128894e-01,\n",
              "        -6.46063312e-01, -1.90304565e-01,  1.30604405e+00,\n",
              "        -1.18367503e+00, -3.52715696e-01, -8.00082026e-01,\n",
              "        -1.07384956e+00,  2.01071808e-01,  9.40316196e-01,\n",
              "         6.87932088e-01, -7.03290034e-01],\n",
              "       [ 0.00000000e+00,  2.33795902e-01, -6.81454515e-01,\n",
              "        -6.87688169e-01, -8.16967364e-01, -3.97230661e-01,\n",
              "         1.60704723e-02,  4.49604691e-01,  2.07457831e-01,\n",
              "         1.57924212e-01, -6.54092159e-01, -1.04797847e+00,\n",
              "        -9.86837029e-01,  2.33024220e-01,  5.82293107e-01,\n",
              "         1.09920729e-01,  2.79280236e-01],\n",
              "       [ 0.00000000e+00,  9.67010050e-01,  5.92190324e-01,\n",
              "        -1.60134940e-01,  7.85945331e-01,  7.43785355e-01,\n",
              "         2.05035108e+00,  1.65930132e+00,  2.83900447e+00,\n",
              "         3.46927657e-01, -6.77990657e-01, -1.13471184e+00,\n",
              "        -1.14960456e+00, -1.17717122e+00, -1.48612716e+00,\n",
              "        -9.34901816e-01, -1.93127764e+00],\n",
              "       [ 0.00000000e+00,  1.32428993e+00,  9.74762653e-01,\n",
              "         1.24225016e+00,  5.71927486e-01,  6.66421358e-01,\n",
              "         7.39581515e-02, -1.62702377e-01,  1.85633830e-02,\n",
              "         9.77620742e-01,  6.16861544e-01,  9.51304359e-01,\n",
              "         2.49648341e-01, -1.55896437e+00, -8.28856627e-01,\n",
              "        -8.96238570e-01, -5.30224822e-01],\n",
              "       [ 0.00000000e+00, -6.65157031e-02, -2.62974976e-02,\n",
              "        -5.71496652e-01, -3.14535757e-01, -1.23518680e-02,\n",
              "         1.48427189e-01,  1.24108888e+00,  9.24923928e-01,\n",
              "        -3.41296253e-01, -3.21207467e-01, -1.34009256e+00,\n",
              "        -9.28935063e-01,  2.93150662e-02,  2.28815309e-02,\n",
              "        -3.20261004e-01, -3.72639715e-01],\n",
              "       [ 0.00000000e+00,  6.17256855e-01,  1.16138937e+00,\n",
              "         8.70680579e-01,  1.06448791e+00, -1.23792637e+00,\n",
              "        -3.00654170e-01, -5.90113762e-01, -1.56258670e-01,\n",
              "         1.85043246e+00,  1.10457811e+00,  9.49314666e-01,\n",
              "         7.80497676e-01,  5.11777399e-01, -9.21912890e-01,\n",
              "        -2.71839828e-01, -7.32274396e-01],\n",
              "       [ 0.00000000e+00, -1.86649888e+00, -6.30527467e-01,\n",
              "        -1.44146283e+00,  2.26959706e-01, -1.07812486e+00,\n",
              "        -5.32379344e-01, -4.11439147e-01, -1.01909751e+00,\n",
              "        -5.21323533e-01, -3.82403965e-01, -9.35456029e-01,\n",
              "         1.02749059e+00,  2.11414701e+00,  7.25875706e-01,\n",
              "         1.12387939e+00,  5.30962926e-01],\n",
              "       [ 0.00000000e+00, -8.79833056e-01, -1.58517349e+00,\n",
              "        -1.25666085e+00,  1.56746659e-01, -1.00012269e+00,\n",
              "        -8.64677421e-01, -1.86816183e-01, -9.50618836e-01,\n",
              "        -2.28567042e-01, -4.24644562e-01, -9.39701273e-01,\n",
              "         8.05551123e-01,  1.42072640e+00,  1.52409490e+00,\n",
              "         8.93199034e-01,  4.75781898e-01],\n",
              "       [ 0.00000000e+00, -1.30472089e+00, -1.50666310e-01,\n",
              "        -5.87347818e-02, -5.84355629e-01,  2.04033660e-01,\n",
              "         6.95581072e-01, -3.34168321e-01, -3.24389028e-01,\n",
              "        -9.20302317e-01, -7.01205634e-01,  8.99563148e-02,\n",
              "        -2.07460732e-01,  8.49535959e-01, -1.05994863e-01,\n",
              "         1.85497157e-01,  5.82108951e-01],\n",
              "       [ 1.00000000e+00,  1.10071440e+00, -5.43181090e-01,\n",
              "        -6.14524242e-01, -3.55866420e-01, -9.39853605e-01,\n",
              "         6.14728254e-01,  1.78261567e+00,  2.11413120e+00,\n",
              "         1.63723554e+00, -9.48836874e-01, -1.58057844e+00,\n",
              "        -1.65809843e+00, -4.51535403e-01,  3.61965675e-02,\n",
              "        -4.71306541e-01, -9.16469023e-01],\n",
              "       [ 1.00000000e+00,  7.51570644e-01,  6.02118498e-01,\n",
              "        -2.05069094e-01, -8.73463228e-01, -7.06586084e-01,\n",
              "         6.01579559e-01,  1.62367468e+00,  1.11040647e+00,\n",
              "         1.21176770e+00, -2.33756054e-02, -1.26936662e+00,\n",
              "        -1.53333158e+00, -1.13116025e-01, -7.69733037e-01,\n",
              "        -7.37930101e-01, -1.28470769e-01],\n",
              "       [ 1.00000000e+00,  1.02542433e+00,  2.59377322e-01,\n",
              "        -5.91697540e-01, -6.95489757e-01,  1.82609198e+00,\n",
              "         2.49816550e+00,  9.24929138e-01,  3.37028002e-01,\n",
              "        -8.60112790e-02, -1.10339806e+00, -1.24239562e+00,\n",
              "        -8.48168739e-01, -1.68918711e+00, -1.20494073e+00,\n",
              "        -2.02993239e-01,  2.02023192e-01],\n",
              "       [ 1.00000000e+00, -9.44229689e-01, -1.11288073e-01,\n",
              "        -3.04916461e-01, -5.45951306e-01, -5.57641576e-01,\n",
              "        -4.65833702e-01, -1.34435120e-02, -2.14278320e-01,\n",
              "        -6.25358411e-01, -1.15754230e-02, -3.81498554e-01,\n",
              "        -2.41502562e-01,  1.05291194e+00,  4.47851069e-01,\n",
              "         5.71360681e-02,  4.67265342e-01],\n",
              "       [ 1.00000000e+00,  6.80121241e-01, -2.09394436e-01,\n",
              "        -6.91215754e-01, -3.03723194e-02, -5.35712292e-02,\n",
              "        -9.89759415e-02, -3.66560683e-01, -7.47914293e-01,\n",
              "         3.24840678e-01, -4.50396759e-01, -4.93887158e-01,\n",
              "         5.68074011e-01, -4.64166128e-01,  1.83416295e-01,\n",
              "         5.66908674e-01,  4.80611256e-01],\n",
              "       [ 1.00000000e+00, -8.72872129e-01, -1.21355225e+00,\n",
              "        -1.04754499e+00,  2.63096402e-01,  2.39309437e-01,\n",
              "         1.46890654e+00,  1.45086308e+00,  1.64725379e+00,\n",
              "        -1.19437613e+00, -1.92472196e+00, -1.87026437e+00,\n",
              "        -8.53432773e-01,  5.93335128e-01,  8.50778758e-03,\n",
              "        -1.62280985e-01, -1.06836733e+00],\n",
              "       [ 1.00000000e+00,  3.25343123e-01, -3.14540830e-01,\n",
              "         1.90059588e-01,  8.81657467e-01, -5.32014024e-01,\n",
              "        -1.00699181e-02,  9.59808387e-02,  5.71581178e-01,\n",
              "         8.40038736e-01, -5.49626729e-01, -8.21472992e-02,\n",
              "         1.95643285e-01,  7.63065478e-02,  1.71021456e-01,\n",
              "        -2.35446004e-01, -9.90075787e-01],\n",
              "       [ 1.00000000e+00,  9.33489314e-01,  1.04378895e+00,\n",
              "         1.44797929e+00,  1.61120939e+00,  1.18421426e+00,\n",
              "         2.78304676e-01,  1.42120492e+00,  7.83535780e-01,\n",
              "        -6.17267726e-02,  8.23926295e-01,  2.69623252e-01,\n",
              "         5.80872949e-01, -1.35808293e+00, -1.30508154e+00,\n",
              "        -1.89289388e+00, -1.62688834e+00],\n",
              "       [ 1.00000000e+00,  9.50073409e-01,  1.41438030e+00,\n",
              "         5.72885252e-01,  9.62496674e-01, -1.05257590e+00,\n",
              "        -3.86675133e-01, -9.38478930e-01, -6.47486794e-01,\n",
              "         1.83809264e+00,  1.54620227e+00,  1.66313558e+00,\n",
              "         1.17123021e+00, -2.26716142e-01, -1.04902974e+00,\n",
              "         9.48909474e-02, -3.52615980e-01],\n",
              "       [ 1.00000000e+00, -1.32443031e+00, -9.66279222e-01,\n",
              "        -1.03547240e+00, -9.54649855e-01, -6.34969601e-01,\n",
              "        -6.85179976e-01, -8.94330744e-01, -3.83179546e-01,\n",
              "        -7.09197568e-01, -4.33953812e-01, -2.47673135e-01,\n",
              "        -6.22365980e-01,  1.09211593e+00,  9.98668004e-01,\n",
              "         1.20240182e+00,  6.83342397e-01],\n",
              "       [ 0.00000000e+00,  4.26226424e-01,  6.21824179e-01,\n",
              "        -2.88407403e-01,  1.68823202e+00,  7.50655455e-01,\n",
              "         1.26115794e+00,  2.31376291e+00,  3.71438691e+00,\n",
              "        -1.79251135e-01, -3.72480509e-01, -1.68306321e+00,\n",
              "        -7.86182003e-01, -4.75089428e-01, -1.02320912e+00,\n",
              "        -1.01988487e+00, -2.85116529e+00],\n",
              "       [ 0.00000000e+00,  6.19577480e-01,  1.23560115e+00,\n",
              "         8.72637377e-01,  3.86350778e-01, -5.44346099e-01,\n",
              "        -7.25531412e-01, -6.16373839e-03, -5.98662283e-01,\n",
              "         5.64474323e-01,  1.72947980e+00,  4.77721573e-01,\n",
              "         6.23677136e-01, -3.64447625e-02, -5.11466024e-01,\n",
              "        -6.70471564e-01,  8.57919465e-03],\n",
              "       [ 0.00000000e+00, -1.75092301e-02,  1.86561270e-01,\n",
              "         7.40772420e-01,  4.85857028e-01, -1.70046069e+00,\n",
              "        -1.34271498e+00, -1.09590403e+00, -1.09502293e+00,\n",
              "         1.25879164e+00,  1.08488877e+00,  1.31436953e+00,\n",
              "         1.32650184e+00,  1.55008825e+00,  7.92961760e-01,\n",
              "         2.03066704e-01,  4.51404758e-01],\n",
              "       [ 1.00000000e+00, -1.04021282e+00, -6.61481578e-01,\n",
              "        -3.84693595e-01,  3.50970108e-01,  1.04666364e+00,\n",
              "         1.48875101e+00,  1.13804813e-01,  4.76386235e-01,\n",
              "        -1.69811114e+00, -1.58229034e+00, -5.77734420e-01,\n",
              "         1.54141111e-03,  1.32663051e-01, -2.77678554e-01,\n",
              "         1.05749536e-01, -4.72146187e-01],\n",
              "       [ 1.00000000e+00,  3.67871722e-01,  4.03714432e-01,\n",
              "        -1.08361943e-01,  1.05082675e-01, -8.59728414e-01,\n",
              "        -8.11656754e-01, -1.21234764e+00, -6.57667870e-01,\n",
              "         9.29205820e-01,  5.85745988e-01,  8.93712384e-01,\n",
              "         6.70453688e-01,  1.70531081e-01,  2.23330533e-01,\n",
              "         9.85126679e-01,  4.05376490e-01],\n",
              "       [ 1.00000000e+00, -1.06162835e+00, -8.58982135e-01,\n",
              "        -1.74095152e+00, -1.03195055e+00, -4.69450353e-01,\n",
              "        -3.35160868e-01, -6.81702114e-01, -1.36638291e+00,\n",
              "        -8.87255055e-01, -6.48523378e-01, -8.93196488e-01,\n",
              "         4.77928410e-01,  1.03274764e+00,  7.26362995e-01,\n",
              "         1.51935258e+00,  1.66347282e+00],\n",
              "       [ 1.00000000e+00, -2.11032777e+00, -2.13845803e+00,\n",
              "        -9.27007355e-01, -3.63106028e-01,  2.39596771e-03,\n",
              "        -9.31913126e-01, -1.43408083e-01, -6.24416615e-01,\n",
              "        -1.73385717e+00, -8.86764199e-01, -7.35697118e-01,\n",
              "         1.55599820e-01,  1.33958307e+00,  1.92183499e+00,\n",
              "         6.74125782e-01,  5.58640329e-01],\n",
              "       [ 1.00000000e+00, -6.44871144e-01,  4.03997229e-01,\n",
              "         5.32572771e-01,  6.71453542e-01, -8.31412808e-01,\n",
              "         2.89492798e-01, -4.58001254e-01, -2.19751483e-01,\n",
              "        -1.79871419e-01, -1.04201499e-01,  6.45214546e-01,\n",
              "         5.32688666e-01,  1.08756800e+00, -5.21578910e-01,\n",
              "        -1.78734711e-01, -4.53319391e-01],\n",
              "       [ 1.00000000e+00, -1.46253874e+00, -8.76838998e-01,\n",
              "        -6.97017124e-01, -4.08372295e-01, -1.96528392e-01,\n",
              "         5.47915906e-02,  1.56020076e+00,  1.63345557e+00,\n",
              "        -1.30870434e+00, -8.91123987e-01, -1.65083386e+00,\n",
              "        -1.48492793e+00,  1.03269905e+00,  5.88587697e-01,\n",
              "        -4.33950960e-01, -6.90928028e-01],\n",
              "       [ 1.00000000e+00, -2.65027114e-01, -8.01537078e-01,\n",
              "         3.00082539e-01, -4.48300997e-01, -5.25294791e-01,\n",
              "        -6.18022654e-01, -5.59807012e-01, -8.77746823e-01,\n",
              "        -3.01611832e-01, -3.63872839e-01,  4.55482177e-01,\n",
              "         2.44384995e-01,  6.03088624e-01,  9.56346264e-01,\n",
              "         7.22078299e-02,  7.36718932e-01],\n",
              "       [ 1.00000000e+00, -1.47887387e+00, -2.23370015e+00,\n",
              "        -1.05720335e+00, -1.06496283e+00, -9.58828151e-01,\n",
              "        -9.88378176e-01, -9.48113655e-01, -1.62557509e-01,\n",
              "        -6.73536020e-01, -1.01136629e+00, -1.62753228e-02,\n",
              "        -7.64030988e-01,  1.62510667e+00,  1.90717341e+00,\n",
              "         1.43206530e+00,  7.09673753e-01],\n",
              "       [ 0.00000000e+00, -1.09552779e+00, -9.45336311e-01,\n",
              "        -8.30556171e-01, -5.99212980e-02, -1.00482463e+00,\n",
              "        -2.66170441e-01,  8.66210123e-01,  1.15560059e+00,\n",
              "        -3.72104280e-01, -7.65833040e-01, -1.37492976e+00,\n",
              "        -9.26337934e-01,  1.46828587e+00,  7.84367117e-01,\n",
              "        -3.73796420e-02, -6.58715095e-01],\n",
              "       [ 0.00000000e+00,  8.66858589e-01,  3.30009177e-01,\n",
              "         4.77060383e-01,  8.12944298e-01, -4.44063965e-01,\n",
              "         5.17653201e-02, -3.81652089e-01, -4.76224730e-01,\n",
              "         9.79583124e-01,  2.94525509e-01,  5.33739312e-01,\n",
              "         1.23929641e+00, -3.91898728e-01, -1.29084894e-01,\n",
              "        -1.87671487e-01, -5.37256074e-02],\n",
              "       [ 0.00000000e+00, -5.31464874e-01,  8.07911560e-01,\n",
              "         4.07889296e-01,  6.22332703e-01, -4.14398021e-01,\n",
              "         3.02184747e-01, -5.09361505e-01, -7.16630211e-02,\n",
              "        -3.86539975e-01,  2.16587962e-01,  5.38836199e-01,\n",
              "         4.23143860e-01,  5.39618325e-01, -8.25866314e-01,\n",
              "        -8.25474003e-02, -4.55590473e-01],\n",
              "       [ 0.00000000e+00, -2.40378520e-01, -5.06815124e-02,\n",
              "        -6.27086800e-01, -5.36871573e-01, -1.06611648e+00,\n",
              "         6.80283494e-02,  5.44845132e-01,  7.78690942e-01,\n",
              "         5.75657900e-01, -1.40961074e-01, -7.61794411e-01,\n",
              "        -1.03894329e+00,  1.27139717e+00,  2.10731241e-01,\n",
              "         1.28769856e-01, -1.67320702e-01],\n",
              "       [ 0.00000000e+00,  3.66230060e-02, -6.18917856e-01,\n",
              "        -2.50219219e-01, -3.26046087e-01, -3.43314978e-01,\n",
              "         6.48212197e-01, -6.30757667e-01,  1.37336456e-01,\n",
              "        -7.32573517e-02, -1.14319753e+00,  6.23331964e-02,\n",
              "        -4.61250727e-01,  3.71327325e-01,  7.92353668e-02,\n",
              "         5.14518431e-01,  1.76794962e-02],\n",
              "       [ 0.00000000e+00, -1.52450591e+00, -7.33500021e-01,\n",
              "        -1.55714080e+00, -3.37539813e-01,  9.41783470e-02,\n",
              "         1.24569922e+00,  4.88625886e-01,  1.02898302e+00,\n",
              "        -1.58632102e+00, -1.55847795e+00, -1.69411937e+00,\n",
              "        -1.04789604e+00,  9.30343697e-01, -1.35993453e-01,\n",
              "         6.11571751e-01, -4.46358055e-01],\n",
              "       [ 1.00000000e+00,  3.66657825e-01, -3.13690502e-01,\n",
              "        -8.79590103e-01,  2.18193005e-01,  2.11936542e+00,\n",
              "         1.19782505e+00,  1.86206509e+00,  1.48925285e+00,\n",
              "        -1.07223972e+00, -1.24742800e+00, -1.84624920e+00,\n",
              "        -9.32169194e-01, -1.21072944e+00, -3.44266824e-01,\n",
              "        -3.62231292e-01, -1.04361834e+00],\n",
              "       [ 1.00000000e+00,  1.27637945e-01, -5.97272066e-01,\n",
              "        -9.19013883e-01, -3.98122038e-01,  1.88551235e-01,\n",
              "         1.13518865e+00,  1.57438660e+00,  1.11158665e+00,\n",
              "        -4.68498690e-01, -1.38675811e+00, -1.74640958e+00,\n",
              "        -1.17734282e+00, -6.62573223e-02, -1.41748319e-01,\n",
              "        -2.22413747e-01, -4.75932527e-01],\n",
              "       [ 1.00000000e+00, -1.33248691e+00, -1.44118395e+00,\n",
              "        -1.10598291e+00, -1.59090730e+00, -1.06985121e+00,\n",
              "        -1.22359636e+00, -6.24456552e-01, -1.39602471e+00,\n",
              "        -3.45307659e-01, -3.17077241e-01, -2.90184284e-01,\n",
              "        -9.11865911e-02,  1.53307696e+00,  1.72140196e+00,\n",
              "         1.24896057e+00,  2.02402915e+00],\n",
              "       [ 1.00000000e+00, -5.39450756e-01,  3.77140264e-01,\n",
              "         8.06568943e-01,  3.49471718e-01,  4.61457954e+00,\n",
              "         3.14480856e+00,  2.23827960e+00,  2.54597471e+00,\n",
              "        -2.66384960e+00, -1.17193567e+00, -7.13715043e-01,\n",
              "        -1.28247154e+00, -1.48612132e+00, -1.71591875e+00,\n",
              "        -1.71196127e+00, -1.50714094e+00],\n",
              "       [ 0.00000000e+00, -1.49289443e-01,  5.49296693e-01,\n",
              "         2.62146301e-02,  1.60392983e-01, -3.15864155e-01,\n",
              "        -5.50540836e-01, -2.37419116e-01, -7.12930989e-02,\n",
              "        -2.36790278e-01,  6.34481235e-01,  1.47892906e-01,\n",
              "         6.62328443e-03,  3.71899781e-01, -1.35821711e-01,\n",
              "         7.43844098e-02, -2.24129668e-01],\n",
              "       [ 0.00000000e+00, -1.89734234e+00, -9.25352688e-01,\n",
              "         1.73016484e-03,  1.39327619e-01,  6.26682305e-01,\n",
              "         1.68257465e+00,  1.59372443e-01, -5.51365474e-02,\n",
              "        -2.05174702e+00, -1.78871970e+00, -3.37664656e-01,\n",
              "         5.72704612e-03,  8.28063019e-01, -1.36309492e-01,\n",
              "        -2.01205093e-01, -1.87336150e-01],\n",
              "       [ 0.00000000e+00,  1.11870126e-02,  2.03730590e-01,\n",
              "         4.00048264e-02,  4.55844178e-01, -1.34437795e+00,\n",
              "        -5.05358904e-01, -1.36297389e+00, -5.96334098e-01,\n",
              "         1.01268518e+00,  1.50935464e-01,  1.20286082e+00,\n",
              "         7.59802184e-01,  1.08932709e+00,  1.64540554e-01,\n",
              "         1.05783307e+00,  2.79158562e-02]])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def bootstrap_mean_diff(x, y, column_index, n_bootstrap=1000):\n",
        "    np.random.seed(42)\n",
        "    mean_diffs = []\n",
        "\n",
        "    for _ in range(n_bootstrap):\n",
        "        bootstrap_indices = np.random.randint(low=0, high=len(x), size=len(x))\n",
        "\n",
        "        bootstrap_x = x[bootstrap_indices]\n",
        "        bootstrap_y = y[bootstrap_indices]\n",
        "\n",
        "        # Separating the data based on genotype\n",
        "        data_1 = bootstrap_x[bootstrap_y == 0, column_index]\n",
        "        data_2 = bootstrap_x[bootstrap_y == 1, column_index]\n",
        "\n",
        "        mean_1 = np.mean(data_1)\n",
        "        mean_2 = np.mean(data_2)\n",
        "\n",
        "        mean_diff = mean_2 - mean_1\n",
        "        mean_diffs.append(mean_diff)\n",
        "\n",
        "    confidence_interval = np.percentile(mean_diffs, [2.5, 97.5])  # 95% of CI\n",
        "\n",
        "    return np.mean(mean_diffs), confidence_interval"
      ],
      "metadata": {
        "id": "KxIW6OwS-C23"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean_diff, ci = bootstrap_mean_diff(x, y, 11)  # index 1 for HR2 (we can change it)\n",
        "print(f\"Mean difference in HR1 between genotypes: {mean_diff}\")\n",
        "print(f\"95% CI for mean difference: {ci}\")"
      ],
      "metadata": {
        "id": "ULu8HduI-kgQ",
        "outputId": "661f5ace-7130-4708-f44a-3a4dd0a1cee4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean difference in HR1 between genotypes: 0.029257820310830428\n",
            "95% CI for mean difference: [-0.48917023  0.52229863]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "id": "zOBehLJHZn9O",
        "outputId": "cd0ddc70-9989-43f4-e300-5d53895eb0ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    Sex       HR2       HR1     HR0.5     HR0.2      FAR2      FAR1    FAR0.5  \\\n",
              "0     1 -0.321007  0.145900  0.283670  0.375000 -1.360401 -1.399860 -1.466841   \n",
              "1     1  0.838200  0.900156 -0.028607  0.366859  1.526202  0.157248  0.299182   \n",
              "2     1  0.631221  0.141229  0.256052  0.082900  1.055317 -0.875904 -0.560909   \n",
              "3     1 -1.821413 -0.630809 -0.877705  0.249481 -0.721763 -0.901431 -0.296571   \n",
              "4     0 -1.124727 -0.937018 -1.183746 -0.604176  0.149710  0.285099  0.086082   \n",
              "5     0  0.328463 -0.894025 -0.544964  0.316214  1.125935  0.520966  1.955051   \n",
              "6     0 -1.626723 -1.600362 -0.276503 -0.505995 -0.499644 -0.658537 -1.105003   \n",
              "7     1 -2.956079 -2.539205 -2.524276 -1.627817  0.073657  0.123426 -0.330639   \n",
              "8     1 -0.540032  0.824311  0.321577 -0.254521  0.877994  1.742419  0.377017   \n",
              "9     1 -1.586274 -1.405576 -2.101618 -0.784668  1.005251  1.060724  0.800544   \n",
              "10    1 -0.812642 -0.387817  0.806648  0.599450 -0.018162 -0.714141  0.486521   \n",
              "11    0  0.283902  0.710435  0.968306  1.798096  1.072047  0.844435  1.426867   \n",
              "12    0  0.721924  0.671542  0.072238  0.918717 -0.338307  0.641473  0.718158   \n",
              "13    0 -0.610836 -0.787440 -1.001875 -0.135965  0.561129 -0.646063 -0.190305   \n",
              "14    0  0.233796 -0.681455 -0.687688 -0.816967 -0.397231  0.016070  0.449605   \n",
              "15    0  0.967010  0.592190 -0.160135  0.785945  0.743785  2.050351  1.659301   \n",
              "16    0  1.324290  0.974763  1.242250  0.571927  0.666421  0.073958 -0.162702   \n",
              "17    0 -0.066516 -0.026297 -0.571497 -0.314536 -0.012352  0.148427  1.241089   \n",
              "18    0  0.617257  1.161389  0.870681  1.064488 -1.237926 -0.300654 -0.590114   \n",
              "19    0 -1.866499 -0.630527 -1.441463  0.226960 -1.078125 -0.532379 -0.411439   \n",
              "20    0 -0.879833 -1.585173 -1.256661  0.156747 -1.000123 -0.864677 -0.186816   \n",
              "21    0 -1.304721 -0.150666 -0.058735 -0.584356  0.204034  0.695581 -0.334168   \n",
              "22    1  1.100714 -0.543181 -0.614524 -0.355866 -0.939854  0.614728  1.782616   \n",
              "23    1  0.751571  0.602118 -0.205069 -0.873463 -0.706586  0.601580  1.623675   \n",
              "24    1  1.025424  0.259377 -0.591698 -0.695490  1.826092  2.498165  0.924929   \n",
              "25    1 -0.944230 -0.111288 -0.304916 -0.545951 -0.557642 -0.465834 -0.013444   \n",
              "26    1  0.680121 -0.209394 -0.691216 -0.030372 -0.053571 -0.098976 -0.366561   \n",
              "27    1 -0.872872 -1.213552 -1.047545  0.263096  0.239309  1.468907  1.450863   \n",
              "28    1  0.325343 -0.314541  0.190060  0.881657 -0.532014 -0.010070  0.095981   \n",
              "29    1  0.933489  1.043789  1.447979  1.611209  1.184214  0.278305  1.421205   \n",
              "30    1  0.950073  1.414380  0.572885  0.962497 -1.052576 -0.386675 -0.938479   \n",
              "31    1 -1.324430 -0.966279 -1.035472 -0.954650 -0.634970 -0.685180 -0.894331   \n",
              "32    0  0.426226  0.621824 -0.288407  1.688232  0.750655  1.261158  2.313763   \n",
              "33    0  0.619577  1.235601  0.872637  0.386351 -0.544346 -0.725531 -0.006164   \n",
              "34    0 -0.017509  0.186561  0.740772  0.485857 -1.700461 -1.342715 -1.095904   \n",
              "35    1 -1.040213 -0.661482 -0.384694  0.350970  1.046664  1.488751  0.113805   \n",
              "36    1  0.367872  0.403714 -0.108362  0.105083 -0.859728 -0.811657 -1.212348   \n",
              "37    1 -1.061628 -0.858982 -1.740952 -1.031951 -0.469450 -0.335161 -0.681702   \n",
              "38    1 -2.110328 -2.138458 -0.927007 -0.363106  0.002396 -0.931913 -0.143408   \n",
              "39    1 -0.644871  0.403997  0.532573  0.671454 -0.831413  0.289493 -0.458001   \n",
              "40    1 -1.462539 -0.876839 -0.697017 -0.408372 -0.196528  0.054792  1.560201   \n",
              "41    1 -0.265027 -0.801537  0.300083 -0.448301 -0.525295 -0.618023 -0.559807   \n",
              "42    1 -1.478874 -2.233700 -1.057203 -1.064963 -0.958828 -0.988378 -0.948114   \n",
              "43    0 -1.095528 -0.945336 -0.830556 -0.059921 -1.004825 -0.266170  0.866210   \n",
              "44    0  0.866859  0.330009  0.477060  0.812944 -0.444064  0.051765 -0.381652   \n",
              "45    0 -0.531465  0.807912  0.407889  0.622333 -0.414398  0.302185 -0.509362   \n",
              "46    0 -0.240379 -0.050682 -0.627087 -0.536872 -1.066116  0.068028  0.544845   \n",
              "47    0  0.036623 -0.618918 -0.250219 -0.326046 -0.343315  0.648212 -0.630758   \n",
              "48    0 -1.524506 -0.733500 -1.557141 -0.337540  0.094178  1.245699  0.488626   \n",
              "49    1  0.366658 -0.313691 -0.879590  0.218193  2.119365  1.197825  1.862065   \n",
              "50    1  0.127638 -0.597272 -0.919014 -0.398122  0.188551  1.135189  1.574387   \n",
              "51    1 -1.332487 -1.441184 -1.105983 -1.590907 -1.069851 -1.223596 -0.624457   \n",
              "52    1 -0.539451  0.377140  0.806569  0.349472  4.614580  3.144809  2.238280   \n",
              "53    0 -0.149289  0.549297  0.026215  0.160393 -0.315864 -0.550541 -0.237419   \n",
              "54    0 -1.897342 -0.925353  0.001730  0.139328  0.626682  1.682575  0.159372   \n",
              "55    0  0.011187  0.203731  0.040005  0.455844 -1.344378 -0.505359 -1.362974   \n",
              "\n",
              "      FAR0.2       DP2       DP1     DP0.5     DP0.2       CB2       CB1  \\\n",
              "0  -1.613927  0.698692  1.011063  1.481159  2.308162  1.455032  0.910777   \n",
              "1   0.844643 -0.345342  0.424193 -0.404200 -0.427387 -1.415365 -0.805983   \n",
              "2  -0.442962 -0.279917  0.472181  0.453073  0.425985 -1.056289  0.549875   \n",
              "3   0.156652 -0.848695  0.212120 -0.529536  0.089508  1.553309  0.930469   \n",
              "4  -1.218350 -1.380622 -1.178709 -1.183913  0.611473  0.726212  0.390042   \n",
              "5   2.811528 -0.750635 -1.265600 -1.741347 -1.476481 -0.726235  0.245055   \n",
              "6  -1.107796 -0.856082 -0.950056  0.733189  0.741363  1.338104  1.336528   \n",
              "7   0.534861 -2.218190 -2.057806 -1.729127 -1.851838  1.613086  1.319275   \n",
              "8   0.344913 -1.300947 -0.446498 -0.223500 -0.597919  0.060506 -1.440239   \n",
              "9   0.809122 -2.150920 -1.891937 -2.266385 -1.307644  0.525949  0.272157   \n",
              "10  0.382461 -0.948088 -0.047687  0.080656  0.017848  0.563065  0.727815   \n",
              "11  1.374764 -0.503330 -0.190808 -0.154439  0.507453 -0.826627 -0.962751   \n",
              "12  1.500730  0.976688  0.051495 -0.519978 -0.239017 -0.578785 -0.848723   \n",
              "13  1.306044 -1.183675 -0.352716 -0.800082 -1.073850  0.201072  0.940316   \n",
              "14  0.207458  0.157924 -0.654092 -1.047978 -0.986837  0.233024  0.582293   \n",
              "15  2.839004  0.346928 -0.677991 -1.134712 -1.149605 -1.177171 -1.486127   \n",
              "16  0.018563  0.977621  0.616862  0.951304  0.249648 -1.558964 -0.828857   \n",
              "17  0.924924 -0.341296 -0.321207 -1.340093 -0.928935  0.029315  0.022882   \n",
              "18 -0.156259  1.850432  1.104578  0.949315  0.780498  0.511777 -0.921913   \n",
              "19 -1.019098 -0.521324 -0.382404 -0.935456  1.027491  2.114147  0.725876   \n",
              "20 -0.950619 -0.228567 -0.424645 -0.939701  0.805551  1.420726  1.524095   \n",
              "21 -0.324389 -0.920302 -0.701206  0.089956 -0.207461  0.849536 -0.105995   \n",
              "22  2.114131  1.637236 -0.948837 -1.580578 -1.658098 -0.451535  0.036197   \n",
              "23  1.110406  1.211768 -0.023376 -1.269367 -1.533332 -0.113116 -0.769733   \n",
              "24  0.337028 -0.086011 -1.103398 -1.242396 -0.848169 -1.689187 -1.204941   \n",
              "25 -0.214278 -0.625358 -0.011575 -0.381499 -0.241503  1.052912  0.447851   \n",
              "26 -0.747914  0.324841 -0.450397 -0.493887  0.568074 -0.464166  0.183416   \n",
              "27  1.647254 -1.194376 -1.924722 -1.870264 -0.853433  0.593335  0.008508   \n",
              "28  0.571581  0.840039 -0.549627 -0.082147  0.195643  0.076307  0.171021   \n",
              "29  0.783536 -0.061727  0.823926  0.269623  0.580873 -1.358083 -1.305082   \n",
              "30 -0.647487  1.838093  1.546202  1.663136  1.171230 -0.226716 -1.049030   \n",
              "31 -0.383180 -0.709198 -0.433954 -0.247673 -0.622366  1.092116  0.998668   \n",
              "32  3.714387 -0.179251 -0.372481 -1.683063 -0.786182 -0.475089 -1.023209   \n",
              "33 -0.598662  0.564474  1.729480  0.477722  0.623677 -0.036445 -0.511466   \n",
              "34 -1.095023  1.258792  1.084889  1.314370  1.326502  1.550088  0.792962   \n",
              "35  0.476386 -1.698111 -1.582290 -0.577734  0.001541  0.132663 -0.277679   \n",
              "36 -0.657668  0.929206  0.585746  0.893712  0.670454  0.170531  0.223331   \n",
              "37 -1.366383 -0.887255 -0.648523 -0.893196  0.477928  1.032748  0.726363   \n",
              "38 -0.624417 -1.733857 -0.886764 -0.735697  0.155600  1.339583  1.921835   \n",
              "39 -0.219751 -0.179871 -0.104201  0.645215  0.532689  1.087568 -0.521579   \n",
              "40  1.633456 -1.308704 -0.891124 -1.650834 -1.484928  1.032699  0.588588   \n",
              "41 -0.877747 -0.301612 -0.363873  0.455482  0.244385  0.603089  0.956346   \n",
              "42 -0.162558 -0.673536 -1.011366 -0.016275 -0.764031  1.625107  1.907173   \n",
              "43  1.155601 -0.372104 -0.765833 -1.374930 -0.926338  1.468286  0.784367   \n",
              "44 -0.476225  0.979583  0.294526  0.533739  1.239296 -0.391899 -0.129085   \n",
              "45 -0.071663 -0.386540  0.216588  0.538836  0.423144  0.539618 -0.825866   \n",
              "46  0.778691  0.575658 -0.140961 -0.761794 -1.038943  1.271397  0.210731   \n",
              "47  0.137336 -0.073257 -1.143198  0.062333 -0.461251  0.371327  0.079235   \n",
              "48  1.028983 -1.586321 -1.558478 -1.694119 -1.047896  0.930344 -0.135993   \n",
              "49  1.489253 -1.072240 -1.247428 -1.846249 -0.932169 -1.210729 -0.344267   \n",
              "50  1.111587 -0.468499 -1.386758 -1.746410 -1.177343 -0.066257 -0.141748   \n",
              "51 -1.396025 -0.345308 -0.317077 -0.290184 -0.091187  1.533077  1.721402   \n",
              "52  2.545975 -2.663850 -1.171936 -0.713715 -1.282472 -1.486121 -1.715919   \n",
              "53 -0.071293 -0.236790  0.634481  0.147893  0.006623  0.371900 -0.135822   \n",
              "54 -0.055137 -2.051747 -1.788720 -0.337665  0.005727  0.828063 -0.136309   \n",
              "55 -0.596334  1.012685  0.150935  1.202861  0.759802  1.089327  0.164541   \n",
              "\n",
              "       CB0.5     CB0.2  Genotype  \n",
              "0   0.904379  1.379075         1  \n",
              "1  -0.271177 -0.821238         1  \n",
              "2   0.129335  0.226182         0  \n",
              "3   0.768281 -0.248347         0  \n",
              "4   0.593240  1.259802         0  \n",
              "5  -0.694972 -1.616835         1  \n",
              "6   1.101274  1.208530         0  \n",
              "7   1.783171  0.609894         0  \n",
              "8  -0.490613 -0.119471         0  \n",
              "9   0.786734 -0.029954         1  \n",
              "10 -0.927719 -0.754915         1  \n",
              "11 -1.471531 -2.008066         0  \n",
              "12 -0.495087 -1.487506         1  \n",
              "13  0.687932 -0.703290         1  \n",
              "14  0.109921  0.279280         0  \n",
              "15 -0.934902 -1.931278         1  \n",
              "16 -0.896239 -0.530225         0  \n",
              "17 -0.320261 -0.372640         0  \n",
              "18 -0.271840 -0.732274         1  \n",
              "19  1.123879  0.530963         0  \n",
              "20  0.893199  0.475782         0  \n",
              "21  0.185497  0.582109         1  \n",
              "22 -0.471307 -0.916469         1  \n",
              "23 -0.737930 -0.128471         1  \n",
              "24 -0.202993  0.202023         0  \n",
              "25  0.057136  0.467265         1  \n",
              "26  0.566909  0.480611         0  \n",
              "27 -0.162281 -1.068367         0  \n",
              "28 -0.235446 -0.990076         1  \n",
              "29 -1.892894 -1.626888         0  \n",
              "30  0.094891 -0.352616         1  \n",
              "31  1.202402  0.683342         1  \n",
              "32 -1.019885 -2.851165         1  \n",
              "33 -0.670472  0.008579         0  \n",
              "34  0.203067  0.451405         1  \n",
              "35  0.105750 -0.472146         1  \n",
              "36  0.985127  0.405376         0  \n",
              "37  1.519353  1.663473         1  \n",
              "38  0.674126  0.558640         0  \n",
              "39 -0.178735 -0.453319         0  \n",
              "40 -0.433951 -0.690928         0  \n",
              "41  0.072208  0.736719         1  \n",
              "42  1.432065  0.709674         1  \n",
              "43 -0.037380 -0.658715         0  \n",
              "44 -0.187671 -0.053726         1  \n",
              "45 -0.082547 -0.455590         0  \n",
              "46  0.128770 -0.167321         1  \n",
              "47  0.514518  0.017679         1  \n",
              "48  0.611572 -0.446358         1  \n",
              "49 -0.362231 -1.043618         0  \n",
              "50 -0.222414 -0.475933         1  \n",
              "51  1.248961  2.024029         0  \n",
              "52 -1.711961 -1.507141         1  \n",
              "53  0.074384 -0.224130         1  \n",
              "54 -0.201205 -0.187336         0  \n",
              "55  1.057833  0.027916         0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c6085983-8317-4a0f-b8d1-986ad3d3bc3a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sex</th>\n",
              "      <th>HR2</th>\n",
              "      <th>HR1</th>\n",
              "      <th>HR0.5</th>\n",
              "      <th>HR0.2</th>\n",
              "      <th>FAR2</th>\n",
              "      <th>FAR1</th>\n",
              "      <th>FAR0.5</th>\n",
              "      <th>FAR0.2</th>\n",
              "      <th>DP2</th>\n",
              "      <th>DP1</th>\n",
              "      <th>DP0.5</th>\n",
              "      <th>DP0.2</th>\n",
              "      <th>CB2</th>\n",
              "      <th>CB1</th>\n",
              "      <th>CB0.5</th>\n",
              "      <th>CB0.2</th>\n",
              "      <th>Genotype</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>-0.321007</td>\n",
              "      <td>0.145900</td>\n",
              "      <td>0.283670</td>\n",
              "      <td>0.375000</td>\n",
              "      <td>-1.360401</td>\n",
              "      <td>-1.399860</td>\n",
              "      <td>-1.466841</td>\n",
              "      <td>-1.613927</td>\n",
              "      <td>0.698692</td>\n",
              "      <td>1.011063</td>\n",
              "      <td>1.481159</td>\n",
              "      <td>2.308162</td>\n",
              "      <td>1.455032</td>\n",
              "      <td>0.910777</td>\n",
              "      <td>0.904379</td>\n",
              "      <td>1.379075</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0.838200</td>\n",
              "      <td>0.900156</td>\n",
              "      <td>-0.028607</td>\n",
              "      <td>0.366859</td>\n",
              "      <td>1.526202</td>\n",
              "      <td>0.157248</td>\n",
              "      <td>0.299182</td>\n",
              "      <td>0.844643</td>\n",
              "      <td>-0.345342</td>\n",
              "      <td>0.424193</td>\n",
              "      <td>-0.404200</td>\n",
              "      <td>-0.427387</td>\n",
              "      <td>-1.415365</td>\n",
              "      <td>-0.805983</td>\n",
              "      <td>-0.271177</td>\n",
              "      <td>-0.821238</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>0.631221</td>\n",
              "      <td>0.141229</td>\n",
              "      <td>0.256052</td>\n",
              "      <td>0.082900</td>\n",
              "      <td>1.055317</td>\n",
              "      <td>-0.875904</td>\n",
              "      <td>-0.560909</td>\n",
              "      <td>-0.442962</td>\n",
              "      <td>-0.279917</td>\n",
              "      <td>0.472181</td>\n",
              "      <td>0.453073</td>\n",
              "      <td>0.425985</td>\n",
              "      <td>-1.056289</td>\n",
              "      <td>0.549875</td>\n",
              "      <td>0.129335</td>\n",
              "      <td>0.226182</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>-1.821413</td>\n",
              "      <td>-0.630809</td>\n",
              "      <td>-0.877705</td>\n",
              "      <td>0.249481</td>\n",
              "      <td>-0.721763</td>\n",
              "      <td>-0.901431</td>\n",
              "      <td>-0.296571</td>\n",
              "      <td>0.156652</td>\n",
              "      <td>-0.848695</td>\n",
              "      <td>0.212120</td>\n",
              "      <td>-0.529536</td>\n",
              "      <td>0.089508</td>\n",
              "      <td>1.553309</td>\n",
              "      <td>0.930469</td>\n",
              "      <td>0.768281</td>\n",
              "      <td>-0.248347</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>-1.124727</td>\n",
              "      <td>-0.937018</td>\n",
              "      <td>-1.183746</td>\n",
              "      <td>-0.604176</td>\n",
              "      <td>0.149710</td>\n",
              "      <td>0.285099</td>\n",
              "      <td>0.086082</td>\n",
              "      <td>-1.218350</td>\n",
              "      <td>-1.380622</td>\n",
              "      <td>-1.178709</td>\n",
              "      <td>-1.183913</td>\n",
              "      <td>0.611473</td>\n",
              "      <td>0.726212</td>\n",
              "      <td>0.390042</td>\n",
              "      <td>0.593240</td>\n",
              "      <td>1.259802</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0</td>\n",
              "      <td>0.328463</td>\n",
              "      <td>-0.894025</td>\n",
              "      <td>-0.544964</td>\n",
              "      <td>0.316214</td>\n",
              "      <td>1.125935</td>\n",
              "      <td>0.520966</td>\n",
              "      <td>1.955051</td>\n",
              "      <td>2.811528</td>\n",
              "      <td>-0.750635</td>\n",
              "      <td>-1.265600</td>\n",
              "      <td>-1.741347</td>\n",
              "      <td>-1.476481</td>\n",
              "      <td>-0.726235</td>\n",
              "      <td>0.245055</td>\n",
              "      <td>-0.694972</td>\n",
              "      <td>-1.616835</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0</td>\n",
              "      <td>-1.626723</td>\n",
              "      <td>-1.600362</td>\n",
              "      <td>-0.276503</td>\n",
              "      <td>-0.505995</td>\n",
              "      <td>-0.499644</td>\n",
              "      <td>-0.658537</td>\n",
              "      <td>-1.105003</td>\n",
              "      <td>-1.107796</td>\n",
              "      <td>-0.856082</td>\n",
              "      <td>-0.950056</td>\n",
              "      <td>0.733189</td>\n",
              "      <td>0.741363</td>\n",
              "      <td>1.338104</td>\n",
              "      <td>1.336528</td>\n",
              "      <td>1.101274</td>\n",
              "      <td>1.208530</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1</td>\n",
              "      <td>-2.956079</td>\n",
              "      <td>-2.539205</td>\n",
              "      <td>-2.524276</td>\n",
              "      <td>-1.627817</td>\n",
              "      <td>0.073657</td>\n",
              "      <td>0.123426</td>\n",
              "      <td>-0.330639</td>\n",
              "      <td>0.534861</td>\n",
              "      <td>-2.218190</td>\n",
              "      <td>-2.057806</td>\n",
              "      <td>-1.729127</td>\n",
              "      <td>-1.851838</td>\n",
              "      <td>1.613086</td>\n",
              "      <td>1.319275</td>\n",
              "      <td>1.783171</td>\n",
              "      <td>0.609894</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1</td>\n",
              "      <td>-0.540032</td>\n",
              "      <td>0.824311</td>\n",
              "      <td>0.321577</td>\n",
              "      <td>-0.254521</td>\n",
              "      <td>0.877994</td>\n",
              "      <td>1.742419</td>\n",
              "      <td>0.377017</td>\n",
              "      <td>0.344913</td>\n",
              "      <td>-1.300947</td>\n",
              "      <td>-0.446498</td>\n",
              "      <td>-0.223500</td>\n",
              "      <td>-0.597919</td>\n",
              "      <td>0.060506</td>\n",
              "      <td>-1.440239</td>\n",
              "      <td>-0.490613</td>\n",
              "      <td>-0.119471</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1</td>\n",
              "      <td>-1.586274</td>\n",
              "      <td>-1.405576</td>\n",
              "      <td>-2.101618</td>\n",
              "      <td>-0.784668</td>\n",
              "      <td>1.005251</td>\n",
              "      <td>1.060724</td>\n",
              "      <td>0.800544</td>\n",
              "      <td>0.809122</td>\n",
              "      <td>-2.150920</td>\n",
              "      <td>-1.891937</td>\n",
              "      <td>-2.266385</td>\n",
              "      <td>-1.307644</td>\n",
              "      <td>0.525949</td>\n",
              "      <td>0.272157</td>\n",
              "      <td>0.786734</td>\n",
              "      <td>-0.029954</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>1</td>\n",
              "      <td>-0.812642</td>\n",
              "      <td>-0.387817</td>\n",
              "      <td>0.806648</td>\n",
              "      <td>0.599450</td>\n",
              "      <td>-0.018162</td>\n",
              "      <td>-0.714141</td>\n",
              "      <td>0.486521</td>\n",
              "      <td>0.382461</td>\n",
              "      <td>-0.948088</td>\n",
              "      <td>-0.047687</td>\n",
              "      <td>0.080656</td>\n",
              "      <td>0.017848</td>\n",
              "      <td>0.563065</td>\n",
              "      <td>0.727815</td>\n",
              "      <td>-0.927719</td>\n",
              "      <td>-0.754915</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0</td>\n",
              "      <td>0.283902</td>\n",
              "      <td>0.710435</td>\n",
              "      <td>0.968306</td>\n",
              "      <td>1.798096</td>\n",
              "      <td>1.072047</td>\n",
              "      <td>0.844435</td>\n",
              "      <td>1.426867</td>\n",
              "      <td>1.374764</td>\n",
              "      <td>-0.503330</td>\n",
              "      <td>-0.190808</td>\n",
              "      <td>-0.154439</td>\n",
              "      <td>0.507453</td>\n",
              "      <td>-0.826627</td>\n",
              "      <td>-0.962751</td>\n",
              "      <td>-1.471531</td>\n",
              "      <td>-2.008066</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0</td>\n",
              "      <td>0.721924</td>\n",
              "      <td>0.671542</td>\n",
              "      <td>0.072238</td>\n",
              "      <td>0.918717</td>\n",
              "      <td>-0.338307</td>\n",
              "      <td>0.641473</td>\n",
              "      <td>0.718158</td>\n",
              "      <td>1.500730</td>\n",
              "      <td>0.976688</td>\n",
              "      <td>0.051495</td>\n",
              "      <td>-0.519978</td>\n",
              "      <td>-0.239017</td>\n",
              "      <td>-0.578785</td>\n",
              "      <td>-0.848723</td>\n",
              "      <td>-0.495087</td>\n",
              "      <td>-1.487506</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0</td>\n",
              "      <td>-0.610836</td>\n",
              "      <td>-0.787440</td>\n",
              "      <td>-1.001875</td>\n",
              "      <td>-0.135965</td>\n",
              "      <td>0.561129</td>\n",
              "      <td>-0.646063</td>\n",
              "      <td>-0.190305</td>\n",
              "      <td>1.306044</td>\n",
              "      <td>-1.183675</td>\n",
              "      <td>-0.352716</td>\n",
              "      <td>-0.800082</td>\n",
              "      <td>-1.073850</td>\n",
              "      <td>0.201072</td>\n",
              "      <td>0.940316</td>\n",
              "      <td>0.687932</td>\n",
              "      <td>-0.703290</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0</td>\n",
              "      <td>0.233796</td>\n",
              "      <td>-0.681455</td>\n",
              "      <td>-0.687688</td>\n",
              "      <td>-0.816967</td>\n",
              "      <td>-0.397231</td>\n",
              "      <td>0.016070</td>\n",
              "      <td>0.449605</td>\n",
              "      <td>0.207458</td>\n",
              "      <td>0.157924</td>\n",
              "      <td>-0.654092</td>\n",
              "      <td>-1.047978</td>\n",
              "      <td>-0.986837</td>\n",
              "      <td>0.233024</td>\n",
              "      <td>0.582293</td>\n",
              "      <td>0.109921</td>\n",
              "      <td>0.279280</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0</td>\n",
              "      <td>0.967010</td>\n",
              "      <td>0.592190</td>\n",
              "      <td>-0.160135</td>\n",
              "      <td>0.785945</td>\n",
              "      <td>0.743785</td>\n",
              "      <td>2.050351</td>\n",
              "      <td>1.659301</td>\n",
              "      <td>2.839004</td>\n",
              "      <td>0.346928</td>\n",
              "      <td>-0.677991</td>\n",
              "      <td>-1.134712</td>\n",
              "      <td>-1.149605</td>\n",
              "      <td>-1.177171</td>\n",
              "      <td>-1.486127</td>\n",
              "      <td>-0.934902</td>\n",
              "      <td>-1.931278</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0</td>\n",
              "      <td>1.324290</td>\n",
              "      <td>0.974763</td>\n",
              "      <td>1.242250</td>\n",
              "      <td>0.571927</td>\n",
              "      <td>0.666421</td>\n",
              "      <td>0.073958</td>\n",
              "      <td>-0.162702</td>\n",
              "      <td>0.018563</td>\n",
              "      <td>0.977621</td>\n",
              "      <td>0.616862</td>\n",
              "      <td>0.951304</td>\n",
              "      <td>0.249648</td>\n",
              "      <td>-1.558964</td>\n",
              "      <td>-0.828857</td>\n",
              "      <td>-0.896239</td>\n",
              "      <td>-0.530225</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0</td>\n",
              "      <td>-0.066516</td>\n",
              "      <td>-0.026297</td>\n",
              "      <td>-0.571497</td>\n",
              "      <td>-0.314536</td>\n",
              "      <td>-0.012352</td>\n",
              "      <td>0.148427</td>\n",
              "      <td>1.241089</td>\n",
              "      <td>0.924924</td>\n",
              "      <td>-0.341296</td>\n",
              "      <td>-0.321207</td>\n",
              "      <td>-1.340093</td>\n",
              "      <td>-0.928935</td>\n",
              "      <td>0.029315</td>\n",
              "      <td>0.022882</td>\n",
              "      <td>-0.320261</td>\n",
              "      <td>-0.372640</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0</td>\n",
              "      <td>0.617257</td>\n",
              "      <td>1.161389</td>\n",
              "      <td>0.870681</td>\n",
              "      <td>1.064488</td>\n",
              "      <td>-1.237926</td>\n",
              "      <td>-0.300654</td>\n",
              "      <td>-0.590114</td>\n",
              "      <td>-0.156259</td>\n",
              "      <td>1.850432</td>\n",
              "      <td>1.104578</td>\n",
              "      <td>0.949315</td>\n",
              "      <td>0.780498</td>\n",
              "      <td>0.511777</td>\n",
              "      <td>-0.921913</td>\n",
              "      <td>-0.271840</td>\n",
              "      <td>-0.732274</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0</td>\n",
              "      <td>-1.866499</td>\n",
              "      <td>-0.630527</td>\n",
              "      <td>-1.441463</td>\n",
              "      <td>0.226960</td>\n",
              "      <td>-1.078125</td>\n",
              "      <td>-0.532379</td>\n",
              "      <td>-0.411439</td>\n",
              "      <td>-1.019098</td>\n",
              "      <td>-0.521324</td>\n",
              "      <td>-0.382404</td>\n",
              "      <td>-0.935456</td>\n",
              "      <td>1.027491</td>\n",
              "      <td>2.114147</td>\n",
              "      <td>0.725876</td>\n",
              "      <td>1.123879</td>\n",
              "      <td>0.530963</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0</td>\n",
              "      <td>-0.879833</td>\n",
              "      <td>-1.585173</td>\n",
              "      <td>-1.256661</td>\n",
              "      <td>0.156747</td>\n",
              "      <td>-1.000123</td>\n",
              "      <td>-0.864677</td>\n",
              "      <td>-0.186816</td>\n",
              "      <td>-0.950619</td>\n",
              "      <td>-0.228567</td>\n",
              "      <td>-0.424645</td>\n",
              "      <td>-0.939701</td>\n",
              "      <td>0.805551</td>\n",
              "      <td>1.420726</td>\n",
              "      <td>1.524095</td>\n",
              "      <td>0.893199</td>\n",
              "      <td>0.475782</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>0</td>\n",
              "      <td>-1.304721</td>\n",
              "      <td>-0.150666</td>\n",
              "      <td>-0.058735</td>\n",
              "      <td>-0.584356</td>\n",
              "      <td>0.204034</td>\n",
              "      <td>0.695581</td>\n",
              "      <td>-0.334168</td>\n",
              "      <td>-0.324389</td>\n",
              "      <td>-0.920302</td>\n",
              "      <td>-0.701206</td>\n",
              "      <td>0.089956</td>\n",
              "      <td>-0.207461</td>\n",
              "      <td>0.849536</td>\n",
              "      <td>-0.105995</td>\n",
              "      <td>0.185497</td>\n",
              "      <td>0.582109</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>1</td>\n",
              "      <td>1.100714</td>\n",
              "      <td>-0.543181</td>\n",
              "      <td>-0.614524</td>\n",
              "      <td>-0.355866</td>\n",
              "      <td>-0.939854</td>\n",
              "      <td>0.614728</td>\n",
              "      <td>1.782616</td>\n",
              "      <td>2.114131</td>\n",
              "      <td>1.637236</td>\n",
              "      <td>-0.948837</td>\n",
              "      <td>-1.580578</td>\n",
              "      <td>-1.658098</td>\n",
              "      <td>-0.451535</td>\n",
              "      <td>0.036197</td>\n",
              "      <td>-0.471307</td>\n",
              "      <td>-0.916469</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>1</td>\n",
              "      <td>0.751571</td>\n",
              "      <td>0.602118</td>\n",
              "      <td>-0.205069</td>\n",
              "      <td>-0.873463</td>\n",
              "      <td>-0.706586</td>\n",
              "      <td>0.601580</td>\n",
              "      <td>1.623675</td>\n",
              "      <td>1.110406</td>\n",
              "      <td>1.211768</td>\n",
              "      <td>-0.023376</td>\n",
              "      <td>-1.269367</td>\n",
              "      <td>-1.533332</td>\n",
              "      <td>-0.113116</td>\n",
              "      <td>-0.769733</td>\n",
              "      <td>-0.737930</td>\n",
              "      <td>-0.128471</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>1</td>\n",
              "      <td>1.025424</td>\n",
              "      <td>0.259377</td>\n",
              "      <td>-0.591698</td>\n",
              "      <td>-0.695490</td>\n",
              "      <td>1.826092</td>\n",
              "      <td>2.498165</td>\n",
              "      <td>0.924929</td>\n",
              "      <td>0.337028</td>\n",
              "      <td>-0.086011</td>\n",
              "      <td>-1.103398</td>\n",
              "      <td>-1.242396</td>\n",
              "      <td>-0.848169</td>\n",
              "      <td>-1.689187</td>\n",
              "      <td>-1.204941</td>\n",
              "      <td>-0.202993</td>\n",
              "      <td>0.202023</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>1</td>\n",
              "      <td>-0.944230</td>\n",
              "      <td>-0.111288</td>\n",
              "      <td>-0.304916</td>\n",
              "      <td>-0.545951</td>\n",
              "      <td>-0.557642</td>\n",
              "      <td>-0.465834</td>\n",
              "      <td>-0.013444</td>\n",
              "      <td>-0.214278</td>\n",
              "      <td>-0.625358</td>\n",
              "      <td>-0.011575</td>\n",
              "      <td>-0.381499</td>\n",
              "      <td>-0.241503</td>\n",
              "      <td>1.052912</td>\n",
              "      <td>0.447851</td>\n",
              "      <td>0.057136</td>\n",
              "      <td>0.467265</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>1</td>\n",
              "      <td>0.680121</td>\n",
              "      <td>-0.209394</td>\n",
              "      <td>-0.691216</td>\n",
              "      <td>-0.030372</td>\n",
              "      <td>-0.053571</td>\n",
              "      <td>-0.098976</td>\n",
              "      <td>-0.366561</td>\n",
              "      <td>-0.747914</td>\n",
              "      <td>0.324841</td>\n",
              "      <td>-0.450397</td>\n",
              "      <td>-0.493887</td>\n",
              "      <td>0.568074</td>\n",
              "      <td>-0.464166</td>\n",
              "      <td>0.183416</td>\n",
              "      <td>0.566909</td>\n",
              "      <td>0.480611</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>1</td>\n",
              "      <td>-0.872872</td>\n",
              "      <td>-1.213552</td>\n",
              "      <td>-1.047545</td>\n",
              "      <td>0.263096</td>\n",
              "      <td>0.239309</td>\n",
              "      <td>1.468907</td>\n",
              "      <td>1.450863</td>\n",
              "      <td>1.647254</td>\n",
              "      <td>-1.194376</td>\n",
              "      <td>-1.924722</td>\n",
              "      <td>-1.870264</td>\n",
              "      <td>-0.853433</td>\n",
              "      <td>0.593335</td>\n",
              "      <td>0.008508</td>\n",
              "      <td>-0.162281</td>\n",
              "      <td>-1.068367</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>1</td>\n",
              "      <td>0.325343</td>\n",
              "      <td>-0.314541</td>\n",
              "      <td>0.190060</td>\n",
              "      <td>0.881657</td>\n",
              "      <td>-0.532014</td>\n",
              "      <td>-0.010070</td>\n",
              "      <td>0.095981</td>\n",
              "      <td>0.571581</td>\n",
              "      <td>0.840039</td>\n",
              "      <td>-0.549627</td>\n",
              "      <td>-0.082147</td>\n",
              "      <td>0.195643</td>\n",
              "      <td>0.076307</td>\n",
              "      <td>0.171021</td>\n",
              "      <td>-0.235446</td>\n",
              "      <td>-0.990076</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>1</td>\n",
              "      <td>0.933489</td>\n",
              "      <td>1.043789</td>\n",
              "      <td>1.447979</td>\n",
              "      <td>1.611209</td>\n",
              "      <td>1.184214</td>\n",
              "      <td>0.278305</td>\n",
              "      <td>1.421205</td>\n",
              "      <td>0.783536</td>\n",
              "      <td>-0.061727</td>\n",
              "      <td>0.823926</td>\n",
              "      <td>0.269623</td>\n",
              "      <td>0.580873</td>\n",
              "      <td>-1.358083</td>\n",
              "      <td>-1.305082</td>\n",
              "      <td>-1.892894</td>\n",
              "      <td>-1.626888</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>1</td>\n",
              "      <td>0.950073</td>\n",
              "      <td>1.414380</td>\n",
              "      <td>0.572885</td>\n",
              "      <td>0.962497</td>\n",
              "      <td>-1.052576</td>\n",
              "      <td>-0.386675</td>\n",
              "      <td>-0.938479</td>\n",
              "      <td>-0.647487</td>\n",
              "      <td>1.838093</td>\n",
              "      <td>1.546202</td>\n",
              "      <td>1.663136</td>\n",
              "      <td>1.171230</td>\n",
              "      <td>-0.226716</td>\n",
              "      <td>-1.049030</td>\n",
              "      <td>0.094891</td>\n",
              "      <td>-0.352616</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>1</td>\n",
              "      <td>-1.324430</td>\n",
              "      <td>-0.966279</td>\n",
              "      <td>-1.035472</td>\n",
              "      <td>-0.954650</td>\n",
              "      <td>-0.634970</td>\n",
              "      <td>-0.685180</td>\n",
              "      <td>-0.894331</td>\n",
              "      <td>-0.383180</td>\n",
              "      <td>-0.709198</td>\n",
              "      <td>-0.433954</td>\n",
              "      <td>-0.247673</td>\n",
              "      <td>-0.622366</td>\n",
              "      <td>1.092116</td>\n",
              "      <td>0.998668</td>\n",
              "      <td>1.202402</td>\n",
              "      <td>0.683342</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>0</td>\n",
              "      <td>0.426226</td>\n",
              "      <td>0.621824</td>\n",
              "      <td>-0.288407</td>\n",
              "      <td>1.688232</td>\n",
              "      <td>0.750655</td>\n",
              "      <td>1.261158</td>\n",
              "      <td>2.313763</td>\n",
              "      <td>3.714387</td>\n",
              "      <td>-0.179251</td>\n",
              "      <td>-0.372481</td>\n",
              "      <td>-1.683063</td>\n",
              "      <td>-0.786182</td>\n",
              "      <td>-0.475089</td>\n",
              "      <td>-1.023209</td>\n",
              "      <td>-1.019885</td>\n",
              "      <td>-2.851165</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>0</td>\n",
              "      <td>0.619577</td>\n",
              "      <td>1.235601</td>\n",
              "      <td>0.872637</td>\n",
              "      <td>0.386351</td>\n",
              "      <td>-0.544346</td>\n",
              "      <td>-0.725531</td>\n",
              "      <td>-0.006164</td>\n",
              "      <td>-0.598662</td>\n",
              "      <td>0.564474</td>\n",
              "      <td>1.729480</td>\n",
              "      <td>0.477722</td>\n",
              "      <td>0.623677</td>\n",
              "      <td>-0.036445</td>\n",
              "      <td>-0.511466</td>\n",
              "      <td>-0.670472</td>\n",
              "      <td>0.008579</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>0</td>\n",
              "      <td>-0.017509</td>\n",
              "      <td>0.186561</td>\n",
              "      <td>0.740772</td>\n",
              "      <td>0.485857</td>\n",
              "      <td>-1.700461</td>\n",
              "      <td>-1.342715</td>\n",
              "      <td>-1.095904</td>\n",
              "      <td>-1.095023</td>\n",
              "      <td>1.258792</td>\n",
              "      <td>1.084889</td>\n",
              "      <td>1.314370</td>\n",
              "      <td>1.326502</td>\n",
              "      <td>1.550088</td>\n",
              "      <td>0.792962</td>\n",
              "      <td>0.203067</td>\n",
              "      <td>0.451405</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>1</td>\n",
              "      <td>-1.040213</td>\n",
              "      <td>-0.661482</td>\n",
              "      <td>-0.384694</td>\n",
              "      <td>0.350970</td>\n",
              "      <td>1.046664</td>\n",
              "      <td>1.488751</td>\n",
              "      <td>0.113805</td>\n",
              "      <td>0.476386</td>\n",
              "      <td>-1.698111</td>\n",
              "      <td>-1.582290</td>\n",
              "      <td>-0.577734</td>\n",
              "      <td>0.001541</td>\n",
              "      <td>0.132663</td>\n",
              "      <td>-0.277679</td>\n",
              "      <td>0.105750</td>\n",
              "      <td>-0.472146</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>1</td>\n",
              "      <td>0.367872</td>\n",
              "      <td>0.403714</td>\n",
              "      <td>-0.108362</td>\n",
              "      <td>0.105083</td>\n",
              "      <td>-0.859728</td>\n",
              "      <td>-0.811657</td>\n",
              "      <td>-1.212348</td>\n",
              "      <td>-0.657668</td>\n",
              "      <td>0.929206</td>\n",
              "      <td>0.585746</td>\n",
              "      <td>0.893712</td>\n",
              "      <td>0.670454</td>\n",
              "      <td>0.170531</td>\n",
              "      <td>0.223331</td>\n",
              "      <td>0.985127</td>\n",
              "      <td>0.405376</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>1</td>\n",
              "      <td>-1.061628</td>\n",
              "      <td>-0.858982</td>\n",
              "      <td>-1.740952</td>\n",
              "      <td>-1.031951</td>\n",
              "      <td>-0.469450</td>\n",
              "      <td>-0.335161</td>\n",
              "      <td>-0.681702</td>\n",
              "      <td>-1.366383</td>\n",
              "      <td>-0.887255</td>\n",
              "      <td>-0.648523</td>\n",
              "      <td>-0.893196</td>\n",
              "      <td>0.477928</td>\n",
              "      <td>1.032748</td>\n",
              "      <td>0.726363</td>\n",
              "      <td>1.519353</td>\n",
              "      <td>1.663473</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>1</td>\n",
              "      <td>-2.110328</td>\n",
              "      <td>-2.138458</td>\n",
              "      <td>-0.927007</td>\n",
              "      <td>-0.363106</td>\n",
              "      <td>0.002396</td>\n",
              "      <td>-0.931913</td>\n",
              "      <td>-0.143408</td>\n",
              "      <td>-0.624417</td>\n",
              "      <td>-1.733857</td>\n",
              "      <td>-0.886764</td>\n",
              "      <td>-0.735697</td>\n",
              "      <td>0.155600</td>\n",
              "      <td>1.339583</td>\n",
              "      <td>1.921835</td>\n",
              "      <td>0.674126</td>\n",
              "      <td>0.558640</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>1</td>\n",
              "      <td>-0.644871</td>\n",
              "      <td>0.403997</td>\n",
              "      <td>0.532573</td>\n",
              "      <td>0.671454</td>\n",
              "      <td>-0.831413</td>\n",
              "      <td>0.289493</td>\n",
              "      <td>-0.458001</td>\n",
              "      <td>-0.219751</td>\n",
              "      <td>-0.179871</td>\n",
              "      <td>-0.104201</td>\n",
              "      <td>0.645215</td>\n",
              "      <td>0.532689</td>\n",
              "      <td>1.087568</td>\n",
              "      <td>-0.521579</td>\n",
              "      <td>-0.178735</td>\n",
              "      <td>-0.453319</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>1</td>\n",
              "      <td>-1.462539</td>\n",
              "      <td>-0.876839</td>\n",
              "      <td>-0.697017</td>\n",
              "      <td>-0.408372</td>\n",
              "      <td>-0.196528</td>\n",
              "      <td>0.054792</td>\n",
              "      <td>1.560201</td>\n",
              "      <td>1.633456</td>\n",
              "      <td>-1.308704</td>\n",
              "      <td>-0.891124</td>\n",
              "      <td>-1.650834</td>\n",
              "      <td>-1.484928</td>\n",
              "      <td>1.032699</td>\n",
              "      <td>0.588588</td>\n",
              "      <td>-0.433951</td>\n",
              "      <td>-0.690928</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>1</td>\n",
              "      <td>-0.265027</td>\n",
              "      <td>-0.801537</td>\n",
              "      <td>0.300083</td>\n",
              "      <td>-0.448301</td>\n",
              "      <td>-0.525295</td>\n",
              "      <td>-0.618023</td>\n",
              "      <td>-0.559807</td>\n",
              "      <td>-0.877747</td>\n",
              "      <td>-0.301612</td>\n",
              "      <td>-0.363873</td>\n",
              "      <td>0.455482</td>\n",
              "      <td>0.244385</td>\n",
              "      <td>0.603089</td>\n",
              "      <td>0.956346</td>\n",
              "      <td>0.072208</td>\n",
              "      <td>0.736719</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>1</td>\n",
              "      <td>-1.478874</td>\n",
              "      <td>-2.233700</td>\n",
              "      <td>-1.057203</td>\n",
              "      <td>-1.064963</td>\n",
              "      <td>-0.958828</td>\n",
              "      <td>-0.988378</td>\n",
              "      <td>-0.948114</td>\n",
              "      <td>-0.162558</td>\n",
              "      <td>-0.673536</td>\n",
              "      <td>-1.011366</td>\n",
              "      <td>-0.016275</td>\n",
              "      <td>-0.764031</td>\n",
              "      <td>1.625107</td>\n",
              "      <td>1.907173</td>\n",
              "      <td>1.432065</td>\n",
              "      <td>0.709674</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>0</td>\n",
              "      <td>-1.095528</td>\n",
              "      <td>-0.945336</td>\n",
              "      <td>-0.830556</td>\n",
              "      <td>-0.059921</td>\n",
              "      <td>-1.004825</td>\n",
              "      <td>-0.266170</td>\n",
              "      <td>0.866210</td>\n",
              "      <td>1.155601</td>\n",
              "      <td>-0.372104</td>\n",
              "      <td>-0.765833</td>\n",
              "      <td>-1.374930</td>\n",
              "      <td>-0.926338</td>\n",
              "      <td>1.468286</td>\n",
              "      <td>0.784367</td>\n",
              "      <td>-0.037380</td>\n",
              "      <td>-0.658715</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>0</td>\n",
              "      <td>0.866859</td>\n",
              "      <td>0.330009</td>\n",
              "      <td>0.477060</td>\n",
              "      <td>0.812944</td>\n",
              "      <td>-0.444064</td>\n",
              "      <td>0.051765</td>\n",
              "      <td>-0.381652</td>\n",
              "      <td>-0.476225</td>\n",
              "      <td>0.979583</td>\n",
              "      <td>0.294526</td>\n",
              "      <td>0.533739</td>\n",
              "      <td>1.239296</td>\n",
              "      <td>-0.391899</td>\n",
              "      <td>-0.129085</td>\n",
              "      <td>-0.187671</td>\n",
              "      <td>-0.053726</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>0</td>\n",
              "      <td>-0.531465</td>\n",
              "      <td>0.807912</td>\n",
              "      <td>0.407889</td>\n",
              "      <td>0.622333</td>\n",
              "      <td>-0.414398</td>\n",
              "      <td>0.302185</td>\n",
              "      <td>-0.509362</td>\n",
              "      <td>-0.071663</td>\n",
              "      <td>-0.386540</td>\n",
              "      <td>0.216588</td>\n",
              "      <td>0.538836</td>\n",
              "      <td>0.423144</td>\n",
              "      <td>0.539618</td>\n",
              "      <td>-0.825866</td>\n",
              "      <td>-0.082547</td>\n",
              "      <td>-0.455590</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>0</td>\n",
              "      <td>-0.240379</td>\n",
              "      <td>-0.050682</td>\n",
              "      <td>-0.627087</td>\n",
              "      <td>-0.536872</td>\n",
              "      <td>-1.066116</td>\n",
              "      <td>0.068028</td>\n",
              "      <td>0.544845</td>\n",
              "      <td>0.778691</td>\n",
              "      <td>0.575658</td>\n",
              "      <td>-0.140961</td>\n",
              "      <td>-0.761794</td>\n",
              "      <td>-1.038943</td>\n",
              "      <td>1.271397</td>\n",
              "      <td>0.210731</td>\n",
              "      <td>0.128770</td>\n",
              "      <td>-0.167321</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>0</td>\n",
              "      <td>0.036623</td>\n",
              "      <td>-0.618918</td>\n",
              "      <td>-0.250219</td>\n",
              "      <td>-0.326046</td>\n",
              "      <td>-0.343315</td>\n",
              "      <td>0.648212</td>\n",
              "      <td>-0.630758</td>\n",
              "      <td>0.137336</td>\n",
              "      <td>-0.073257</td>\n",
              "      <td>-1.143198</td>\n",
              "      <td>0.062333</td>\n",
              "      <td>-0.461251</td>\n",
              "      <td>0.371327</td>\n",
              "      <td>0.079235</td>\n",
              "      <td>0.514518</td>\n",
              "      <td>0.017679</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>0</td>\n",
              "      <td>-1.524506</td>\n",
              "      <td>-0.733500</td>\n",
              "      <td>-1.557141</td>\n",
              "      <td>-0.337540</td>\n",
              "      <td>0.094178</td>\n",
              "      <td>1.245699</td>\n",
              "      <td>0.488626</td>\n",
              "      <td>1.028983</td>\n",
              "      <td>-1.586321</td>\n",
              "      <td>-1.558478</td>\n",
              "      <td>-1.694119</td>\n",
              "      <td>-1.047896</td>\n",
              "      <td>0.930344</td>\n",
              "      <td>-0.135993</td>\n",
              "      <td>0.611572</td>\n",
              "      <td>-0.446358</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>1</td>\n",
              "      <td>0.366658</td>\n",
              "      <td>-0.313691</td>\n",
              "      <td>-0.879590</td>\n",
              "      <td>0.218193</td>\n",
              "      <td>2.119365</td>\n",
              "      <td>1.197825</td>\n",
              "      <td>1.862065</td>\n",
              "      <td>1.489253</td>\n",
              "      <td>-1.072240</td>\n",
              "      <td>-1.247428</td>\n",
              "      <td>-1.846249</td>\n",
              "      <td>-0.932169</td>\n",
              "      <td>-1.210729</td>\n",
              "      <td>-0.344267</td>\n",
              "      <td>-0.362231</td>\n",
              "      <td>-1.043618</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50</th>\n",
              "      <td>1</td>\n",
              "      <td>0.127638</td>\n",
              "      <td>-0.597272</td>\n",
              "      <td>-0.919014</td>\n",
              "      <td>-0.398122</td>\n",
              "      <td>0.188551</td>\n",
              "      <td>1.135189</td>\n",
              "      <td>1.574387</td>\n",
              "      <td>1.111587</td>\n",
              "      <td>-0.468499</td>\n",
              "      <td>-1.386758</td>\n",
              "      <td>-1.746410</td>\n",
              "      <td>-1.177343</td>\n",
              "      <td>-0.066257</td>\n",
              "      <td>-0.141748</td>\n",
              "      <td>-0.222414</td>\n",
              "      <td>-0.475933</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51</th>\n",
              "      <td>1</td>\n",
              "      <td>-1.332487</td>\n",
              "      <td>-1.441184</td>\n",
              "      <td>-1.105983</td>\n",
              "      <td>-1.590907</td>\n",
              "      <td>-1.069851</td>\n",
              "      <td>-1.223596</td>\n",
              "      <td>-0.624457</td>\n",
              "      <td>-1.396025</td>\n",
              "      <td>-0.345308</td>\n",
              "      <td>-0.317077</td>\n",
              "      <td>-0.290184</td>\n",
              "      <td>-0.091187</td>\n",
              "      <td>1.533077</td>\n",
              "      <td>1.721402</td>\n",
              "      <td>1.248961</td>\n",
              "      <td>2.024029</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52</th>\n",
              "      <td>1</td>\n",
              "      <td>-0.539451</td>\n",
              "      <td>0.377140</td>\n",
              "      <td>0.806569</td>\n",
              "      <td>0.349472</td>\n",
              "      <td>4.614580</td>\n",
              "      <td>3.144809</td>\n",
              "      <td>2.238280</td>\n",
              "      <td>2.545975</td>\n",
              "      <td>-2.663850</td>\n",
              "      <td>-1.171936</td>\n",
              "      <td>-0.713715</td>\n",
              "      <td>-1.282472</td>\n",
              "      <td>-1.486121</td>\n",
              "      <td>-1.715919</td>\n",
              "      <td>-1.711961</td>\n",
              "      <td>-1.507141</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53</th>\n",
              "      <td>0</td>\n",
              "      <td>-0.149289</td>\n",
              "      <td>0.549297</td>\n",
              "      <td>0.026215</td>\n",
              "      <td>0.160393</td>\n",
              "      <td>-0.315864</td>\n",
              "      <td>-0.550541</td>\n",
              "      <td>-0.237419</td>\n",
              "      <td>-0.071293</td>\n",
              "      <td>-0.236790</td>\n",
              "      <td>0.634481</td>\n",
              "      <td>0.147893</td>\n",
              "      <td>0.006623</td>\n",
              "      <td>0.371900</td>\n",
              "      <td>-0.135822</td>\n",
              "      <td>0.074384</td>\n",
              "      <td>-0.224130</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54</th>\n",
              "      <td>0</td>\n",
              "      <td>-1.897342</td>\n",
              "      <td>-0.925353</td>\n",
              "      <td>0.001730</td>\n",
              "      <td>0.139328</td>\n",
              "      <td>0.626682</td>\n",
              "      <td>1.682575</td>\n",
              "      <td>0.159372</td>\n",
              "      <td>-0.055137</td>\n",
              "      <td>-2.051747</td>\n",
              "      <td>-1.788720</td>\n",
              "      <td>-0.337665</td>\n",
              "      <td>0.005727</td>\n",
              "      <td>0.828063</td>\n",
              "      <td>-0.136309</td>\n",
              "      <td>-0.201205</td>\n",
              "      <td>-0.187336</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55</th>\n",
              "      <td>0</td>\n",
              "      <td>0.011187</td>\n",
              "      <td>0.203731</td>\n",
              "      <td>0.040005</td>\n",
              "      <td>0.455844</td>\n",
              "      <td>-1.344378</td>\n",
              "      <td>-0.505359</td>\n",
              "      <td>-1.362974</td>\n",
              "      <td>-0.596334</td>\n",
              "      <td>1.012685</td>\n",
              "      <td>0.150935</td>\n",
              "      <td>1.202861</td>\n",
              "      <td>0.759802</td>\n",
              "      <td>1.089327</td>\n",
              "      <td>0.164541</td>\n",
              "      <td>1.057833</td>\n",
              "      <td>0.027916</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c6085983-8317-4a0f-b8d1-986ad3d3bc3a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-c6085983-8317-4a0f-b8d1-986ad3d3bc3a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-c6085983-8317-4a0f-b8d1-986ad3d3bc3a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-777fbb1e-3a7f-4498-9454-dfa6d1653f4c\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-777fbb1e-3a7f-4498-9454-dfa6d1653f4c')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-777fbb1e-3a7f-4498-9454-dfa6d1653f4c button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "dataset",
              "summary": "{\n  \"name\": \"dataset\",\n  \"rows\": 56,\n  \"fields\": [\n    {\n      \"column\": \"Sex\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"HR2\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.0020142551435303,\n        \"min\": -2.95607864182858,\n        \"max\": 1.32428993438733,\n        \"num_unique_values\": 56,\n        \"samples\": [\n          -0.321006938257828,\n          0.328463389409034\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"HR1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.9144403049448748,\n        \"min\": -2.53920540790395,\n        \"max\": 1.41438029657579,\n        \"num_unique_values\": 56,\n        \"samples\": [\n          0.145900342460394,\n          -0.894024646152531\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"HR0.5\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.8361009223486956,\n        \"min\": -2.52427589958627,\n        \"max\": 1.4479792904615,\n        \"num_unique_values\": 56,\n        \"samples\": [\n          0.283669933919348,\n          -0.544964335454849\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"HR0.2\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.7408019219835157,\n        \"min\": -1.62781662309977,\n        \"max\": 1.79809625090099,\n        \"num_unique_values\": 56,\n        \"samples\": [\n          0.375000391156406,\n          0.316213895931747\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"FAR2\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.0744771297555806,\n        \"min\": -1.70046069360558,\n        \"max\": 4.61457954184583,\n        \"num_unique_values\": 56,\n        \"samples\": [\n          -1.36040050520453,\n          1.12593509756217\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"FAR1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.988209523642952,\n        \"min\": -1.39985980467866,\n        \"max\": 3.1448085550735,\n        \"num_unique_values\": 56,\n        \"samples\": [\n          -1.39985980467866,\n          0.520965773916951\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"FAR0.5\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.9952116067495969,\n        \"min\": -1.46684066757262,\n        \"max\": 2.31376291047421,\n        \"num_unique_values\": 56,\n        \"samples\": [\n          -1.46684066757262,\n          1.95505115928713\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"FAR0.2\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.1649090972395548,\n        \"min\": -1.61392728885408,\n        \"max\": 3.71438691114604,\n        \"num_unique_values\": 56,\n        \"samples\": [\n          -1.61392728885408,\n          2.81152764097226\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"DP2\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.0454728245018383,\n        \"min\": -2.66384960288301,\n        \"max\": 1.85043245743529,\n        \"num_unique_values\": 56,\n        \"samples\": [\n          0.698692337433586,\n          -0.750635235753975\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"DP1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.8710140543346235,\n        \"min\": -2.05780558074746,\n        \"max\": 1.72947979906125,\n        \"num_unique_values\": 56,\n        \"samples\": [\n          1.01106286236846,\n          -1.26559996964455\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"DP0.5\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.9764346294704824,\n        \"min\": -2.26638502172542,\n        \"max\": 1.66313557906917,\n        \"num_unique_values\": 56,\n        \"samples\": [\n          1.48115942152035,\n          -1.74134736277382\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"DP0.2\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.903178802558449,\n        \"min\": -1.85183771929807,\n        \"max\": 2.30816211764708,\n        \"num_unique_values\": 56,\n        \"samples\": [\n          2.30816211764708,\n          -1.47648065213092\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"CB2\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.9761416344873639,\n        \"min\": -1.68918710711977,\n        \"max\": 2.11414700793498,\n        \"num_unique_values\": 56,\n        \"samples\": [\n          1.45503209602801,\n          -0.726234784406469\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"CB1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.886204213820514,\n        \"min\": -1.71591874874384,\n        \"max\": 1.9218349857456,\n        \"num_unique_values\": 56,\n        \"samples\": [\n          0.910777347842817,\n          0.245055462616006\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"CB0.5\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.8015901820328635,\n        \"min\": -1.89289387763231,\n        \"max\": 1.78317148412548,\n        \"num_unique_values\": 56,\n        \"samples\": [\n          0.904378905724626,\n          -0.69497154323295\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"CB0.2\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.9373107558454099,\n        \"min\": -2.85116528744601,\n        \"max\": 2.02402914936401,\n        \"num_unique_values\": 56,\n        \"samples\": [\n          1.37907547292122,\n          -1.61683483017074\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Genotype\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Splitting data into training and test sets"
      ],
      "metadata": {
        "id": "sRS1CGn0cG29"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "s1Kvhh1jcKnh"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building Logistic Regression instance"
      ],
      "metadata": {
        "id": "zi4XjcvTcLzr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bagged_logistic = BaggingClassifier(\n",
        "    estimator=LogisticRegression(max_iter=10000),\n",
        "    n_estimators=10,\n",
        "    random_state=42,\n",
        ")\n",
        "\n",
        "bagged_logistic.fit(x_train, y_train)\n",
        "\n",
        "accuracy = accuracy_score(y_test, bagged_logistic.predict(x_test))\n",
        "print(f\"Accuracy with Bagging: {accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "787FebkQPZbP",
        "outputId": "5f815cb7-034b-4bd8-9720-ad8ab1081073",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with Bagging: 0.6667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = LogisticRegression()"
      ],
      "metadata": {
        "id": "kHIOHq_iECfN"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Perform Grid Search to find the best hyperparameters (tuning)\n"
      ],
      "metadata": {
        "id": "yJ-ytaj1cYyv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid = {\n",
        "    \"C\": [0.001, 0.01, 0.1, 1, 10, 100],\n",
        "    \"solver\": [\"liblinear\", \"newton-cg\", \"saga\", \"sag\", \"lbfgs\"],\n",
        "    \"max_iter\": [100, 500, 1000, 2000, 10000]\n",
        "}"
      ],
      "metadata": {
        "id": "2K63AWL1FfX7"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get the best parameters and estimator from the Grid Search\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "B9T89HdJces_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "grid_search = GridSearchCV(classifier, param_grid, cv=5, scoring=\"accuracy\")\n",
        "\n",
        "grid_search.fit(x_train, y_train)\n",
        "\n",
        "best_params = grid_search.best_params_\n",
        "best_score = grid_search.best_score_\n",
        "print(\"Best Parameters:\", best_params)\n",
        "print(\"Best Score:\", best_score)\n",
        "\n",
        "best_model = grid_search.best_estimator_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LhnuskUWFheN",
        "outputId": "97732e91-3089-436b-ac14-29fb6392507e"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'C': 0.001, 'max_iter': 100, 'solver': 'newton-cg'}\n",
            "Best Score: 0.5222222222222223\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementation of custom function to select accuracy-weighed features"
      ],
      "metadata": {
        "id": "NE1JUbY4clu9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "selected_features = list(range(x_train.shape[1]))\n",
        "initial_accuracy = accuracy_score(y_test, best_model.predict(x_test))\n",
        "\n",
        "for i in selected_features:\n",
        "    features_to_use = [feature for feature in selected_features if feature != i]\n",
        "\n",
        "    if len(features_to_use) > 0:\n",
        "        x_subset = x_train[:, features_to_use]\n",
        "        classifier.fit(x_subset, y_train)\n",
        "        y_pred_subset = classifier.predict(x_test[:, features_to_use])\n",
        "        accuracy_subset = accuracy_score(y_test, y_pred_subset)\n",
        "\n",
        "        if accuracy_subset > initial_accuracy:\n",
        "            print(f\"Removing feature in position {i} - Accuracy improved to {accuracy_subset:.4f}\")\n",
        "            initial_accuracy = accuracy_subset\n",
        "            selected_features = features_to_use\n",
        "        else:\n",
        "            print(f\"Keeping feature in position {i} - Accuracy: {accuracy_subset:.4f}\")\n",
        "    else:\n",
        "        print(f\"All features removed - Terminating Process\")\n",
        "        break\n",
        "\n",
        "print(\"Selected Features:\")\n",
        "for feature_index in selected_features:\n",
        "    if feature_index < len(dataset.columns) - 1:\n",
        "        print(dataset.columns[feature_index])\n",
        "    else:\n",
        "        print(\"Invalid Index\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uja-DIE9FmAr",
        "outputId": "c2b1eb77-205c-4ae6-a90f-b910bce54720"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Removing feature in position 0 - Accuracy improved to 0.5833\n",
            "Keeping feature in position 1 - Accuracy: 0.5833\n",
            "Keeping feature in position 2 - Accuracy: 0.5833\n",
            "Keeping feature in position 3 - Accuracy: 0.5833\n",
            "Keeping feature in position 4 - Accuracy: 0.5833\n",
            "Keeping feature in position 5 - Accuracy: 0.5000\n",
            "Keeping feature in position 6 - Accuracy: 0.5833\n",
            "Keeping feature in position 7 - Accuracy: 0.5833\n",
            "Keeping feature in position 8 - Accuracy: 0.4167\n",
            "Keeping feature in position 9 - Accuracy: 0.5000\n",
            "Keeping feature in position 10 - Accuracy: 0.5833\n",
            "Keeping feature in position 11 - Accuracy: 0.5000\n",
            "Removing feature in position 12 - Accuracy improved to 0.6667\n",
            "Keeping feature in position 13 - Accuracy: 0.6667\n",
            "Keeping feature in position 14 - Accuracy: 0.6667\n",
            "Keeping feature in position 15 - Accuracy: 0.5833\n",
            "Keeping feature in position 16 - Accuracy: 0.6667\n",
            "Selected Features:\n",
            "HR2\n",
            "HR1\n",
            "HR0.5\n",
            "HR0.2\n",
            "FAR2\n",
            "FAR1\n",
            "FAR0.5\n",
            "FAR0.2\n",
            "DP2\n",
            "DP1\n",
            "DP0.5\n",
            "CB2\n",
            "CB1\n",
            "CB0.5\n",
            "CB0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fit the best regressor on the training data\n",
        "\n"
      ],
      "metadata": {
        "id": "P8aUbRjzc0Kh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_model.fit(x_train[:, selected_features], y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "zurnj0JvFtYy",
        "outputId": "20083c96-f9c3-4f74-9dc8-0e36afa405df"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=0.001, solver='newton-cg')"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(C=0.001, solver=&#x27;newton-cg&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(C=0.001, solver=&#x27;newton-cg&#x27;)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Make predictions on the test set using selected features\n",
        "\n"
      ],
      "metadata": {
        "id": "a3TQlpKhc3wS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = best_model.predict(x_test[:, selected_features])"
      ],
      "metadata": {
        "id": "1Eu-nPQ_FwhY"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate performance on test set"
      ],
      "metadata": {
        "id": "17C08phUdKVF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(cm)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KQO5LFmYFzcT",
        "outputId": "c599269c-a4c4-4bae-e0d1-6c17ff2da72a"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 6]\n",
            " [0 6]]\n",
            "Accuracy: 0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building Random Forest classifier instance"
      ],
      "metadata": {
        "id": "6MYVWrD_dVy6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rf_classifier = RandomForestClassifier()"
      ],
      "metadata": {
        "id": "vERmib6DdfPo"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Perform Grid Search to find the best hyperparameters (tuning)\n"
      ],
      "metadata": {
        "id": "i_k5MPBudiA5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid = {\n",
        "    'n_estimators': [100, 300, 500],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}"
      ],
      "metadata": {
        "id": "CplDHv7DdoTc"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get the best parameters and estimator from the Grid Search\n"
      ],
      "metadata": {
        "id": "5SPGAN1vdxB_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "grid_search = GridSearchCV(rf_classifier, param_grid, cv=5, scoring='accuracy')\n",
        "\n",
        "grid_search.fit(x_train, y_train)\n",
        "\n",
        "best_params = grid_search.best_params_\n",
        "best_score = grid_search.best_score_\n",
        "\n",
        "print(\"Best Parameters:\", best_params)\n",
        "print(\"Best Score:\", best_score)\n",
        "\n",
        "best_rf_model = grid_search.best_estimator_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n6Mq6Fqcdv6o",
        "outputId": "c60ed43c-ae2d-498e-c416-540e23476b2f"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': 10, 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 500}\n",
            "Best Score: 0.475\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementation of custom function to select accuracy-weighed features"
      ],
      "metadata": {
        "id": "8NCZKELKeEtw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "selected_features = list(range(x_train.shape[1]))\n",
        "initial_accuracy = accuracy_score(y_test, best_rf_model.predict(x_test))\n",
        "\n",
        "for i in selected_features:\n",
        "    features_to_use = [feature for feature in selected_features if feature != i]\n",
        "\n",
        "    if len(features_to_use) > 0:\n",
        "        x_subset = x_train[:, features_to_use]\n",
        "        rf_classifier.fit(x_subset, y_train)\n",
        "        y_pred_subset = rf_classifier.predict(x_test[:, features_to_use])\n",
        "        accuracy_subset = accuracy_score(y_test, y_pred_subset)\n",
        "\n",
        "        if accuracy_subset > initial_accuracy:\n",
        "            print(f\"Removing feature in position {i} - Accuracy improved to {accuracy_subset:.4f}\")\n",
        "            initial_accuracy = accuracy_subset\n",
        "            selected_features = features_to_use\n",
        "        else:\n",
        "            print(f\"Keeping feature in position {i} - Accuracy: {accuracy_subset:.4f}\")\n",
        "    else:\n",
        "        print(f\"All features removed - Terminating Process\")\n",
        "        break\n",
        "\n",
        "print(\"Selected Features:\")\n",
        "for feature_index in selected_features:\n",
        "    # Assuming 'dataset' has column names\n",
        "    if feature_index < len(dataset.columns) - 1:\n",
        "        print(dataset.columns[feature_index])\n",
        "    else:\n",
        "        print(\"Invalid Index\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k_gEMGoNeENy",
        "outputId": "0a614565-5392-457e-81af-160f4cb011eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Removing feature in position 0 - Accuracy improved to 0.5833\n",
            "Removing feature in position 1 - Accuracy improved to 0.7500\n",
            "Keeping feature in position 2 - Accuracy: 0.6667\n",
            "Keeping feature in position 3 - Accuracy: 0.4167\n",
            "Keeping feature in position 4 - Accuracy: 0.5000\n",
            "Keeping feature in position 5 - Accuracy: 0.6667\n",
            "Keeping feature in position 6 - Accuracy: 0.6667\n",
            "Keeping feature in position 7 - Accuracy: 0.6667\n",
            "Removing feature in position 8 - Accuracy improved to 0.8333\n",
            "Keeping feature in position 9 - Accuracy: 0.6667\n",
            "Keeping feature in position 10 - Accuracy: 0.7500\n",
            "Keeping feature in position 11 - Accuracy: 0.6667\n",
            "Keeping feature in position 12 - Accuracy: 0.5833\n",
            "Keeping feature in position 13 - Accuracy: 0.6667\n",
            "Keeping feature in position 14 - Accuracy: 0.6667\n",
            "Keeping feature in position 15 - Accuracy: 0.6667\n",
            "Keeping feature in position 16 - Accuracy: 0.6667\n",
            "Selected Features:\n",
            "HR1\n",
            "HR0.5\n",
            "HR0.2\n",
            "FAR2\n",
            "FAR1\n",
            "FAR0.5\n",
            "DP2\n",
            "DP1\n",
            "DP0.5\n",
            "DP0.2\n",
            "CB2\n",
            "CB1\n",
            "CB0.5\n",
            "CB0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fit the best regressor on the training data"
      ],
      "metadata": {
        "id": "u-dGHWdweOia"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_rf_model.fit(x_train[:, selected_features], y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "id": "ZW_bTDzZeOG5",
        "outputId": "d517f34c-3942-4066-d9f3-4df9d7ba740f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(min_samples_leaf=2, min_samples_split=10,\n",
              "                       n_estimators=300)"
            ],
            "text/html": [
              "<style>#sk-container-id-33 {color: black;background-color: white;}#sk-container-id-33 pre{padding: 0;}#sk-container-id-33 div.sk-toggleable {background-color: white;}#sk-container-id-33 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-33 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-33 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-33 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-33 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-33 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-33 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-33 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-33 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-33 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-33 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-33 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-33 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-33 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-33 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-33 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-33 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-33 div.sk-item {position: relative;z-index: 1;}#sk-container-id-33 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-33 div.sk-item::before, #sk-container-id-33 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-33 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-33 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-33 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-33 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-33 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-33 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-33 div.sk-label-container {text-align: center;}#sk-container-id-33 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-33 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-33\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(min_samples_leaf=2, min_samples_split=10,\n",
              "                       n_estimators=300)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-33\" type=\"checkbox\" checked><label for=\"sk-estimator-id-33\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(min_samples_leaf=2, min_samples_split=10,\n",
              "                       n_estimators=300)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 313
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Make predictions on the test set using selected features"
      ],
      "metadata": {
        "id": "dZUtMi4feWOB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = best_rf_model.predict(x_test[:, selected_features])"
      ],
      "metadata": {
        "id": "4zFbzc3feRVS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate performance on test set"
      ],
      "metadata": {
        "id": "s_KFdhlGebRn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(cm)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "858-0gbteaqa",
        "outputId": "3076bccd-96df-44f8-e003-3b1f2272e6dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[3 1]\n",
            " [3 5]]\n",
            "Accuracy: 0.6666666666666666\n"
          ]
        }
      ]
    }
  ]
}